{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pickle.load( open( \"../imagenet-200/train_images.pkl\", \"rb\" ) )\n",
    "train_labels = pickle.load( open( \"../imagenet-200/train_labels.pkl\", \"rb\" ) )\n",
    "val_images = pickle.load( open( \"../imagenet-200/val_images.pkl\", \"rb\" ) )\n",
    "val_labels = pickle.load( open( \"../imagenet-200/val_labels.pkl\", \"rb\" ) )\n",
    "y_train = pickle.load( open( \"../imagenet-200/y_train.pkl\", \"rb\" ) )\n",
    "y_test = pickle.load( open( \"../imagenet-200/y_test.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02119789</td>\n",
       "      <td>1</td>\n",
       "      <td>kit_fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02100735</td>\n",
       "      <td>2</td>\n",
       "      <td>English_setter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n02110185</td>\n",
       "      <td>3</td>\n",
       "      <td>Siberian_husky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n02096294</td>\n",
       "      <td>4</td>\n",
       "      <td>Australian_terrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n02102040</td>\n",
       "      <td>5</td>\n",
       "      <td>English_springer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  id         description\n",
       "0  n02119789   1             kit_fox\n",
       "1  n02100735   2      English_setter\n",
       "2  n02110185   3      Siberian_husky\n",
       "3  n02096294   4  Australian_terrier\n",
       "4  n02102040   5    English_springer"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_labels = pd.read_csv('../imagenet-200/map_clsloc.txt', sep='\\s', header=None, engine='python')\n",
    "text_labels.columns=['label', 'id', 'description']\n",
    "text_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAH3NJREFUeJztnVuMXNd1pv9V1dXdVX1ls8lm8yZSFCWG0T0dSZaVRLGRWHE8IxsYGPaDoQfDMgYyMAYyD4IHiD3APDiDsQ0/DDyQx0IUjyJZie1IyQgTS5qMZGFGlChZpC6ULIoiJV66m81ms2916apa81BFgGrvf3eTTVZT3v8HEKzeq/Y5++xzVp2q/Z+1lrk7hBDpkVntAQghVgc5vxCJIucXIlHk/EIkipxfiESR8wuRKHJ+IRJFzi9Eosj5hUiUtpV0NrO7AHwfQBbAf3f3b8fen+8peO/afrYt2q9WrZ332KrVBWrLILKvej3SL0zsKclajY89m81SW1dXF7VVq1Vqm52dDbbHxtjWxi8D49MBM37viB03IxsZR4x65JxZJnyuLfJga+wayJLtLUXk8gbIuYkdVyYTnvu5uSLKpcqyBnnBzm9mWQD/FcCfADgK4CUze8Ld32R9etf244t/+dWgLRc58VOTp4Pt2YgTj42OUls+sq854jwA0G5hZ61HnPH06fDYAWCgfw21jYyMUNvkFN/m888/H2yPfWCsWcPHkZvjH1Dt7e3UNj07E2x35+esPzKO2M1hZn6e2jo7O4LtmYjzz82Hxw4AXflOastEPlGysU9RD39Qlub4cRUK4XH84skX+H4WsZKv/bcAOOjuh9y9AuBRAHevYHtCiBayEuffBOCDc/4+2mwTQnwEuOQLfmZ2r5ntNbO9xVn+NUYI0VpW4vzHAGw55+/NzbYP4e4PuPuIu4/kuwsr2J0Q4mKyEud/CcBOM9tuZu0AvgDgiYszLCHEpeaCV/vdvWpmXwPwz2hIfQ+6+xtL9EF1ISzBsXYAqFQqwfb2LB9+Z46vRLdHbKVMmdq8Hl7NdaICAEBvT1jaBIBTk1PU9n+e/SW1xaS5vt7wirm18THGFIk+59/WauUStZ2cmAy2s3MJACcj4+jt7aW2mETYQ/rVIlJwTMWIybPlEv9Zm42IbwWiSHR0cLk0kwkfc1RSXMSKdH53fxLAkyvZhhBiddATfkIkipxfiESR8wuRKHJ+IRJFzi9Eoqxotf98yWaz6O7uDtoWylwCKmXC8kpM1cjlctQWCxLJRuTDej0cHFMscskrFjQzP1+ktvFTp6gtJjfl8/lgey4iX5Uic19f4MEqsTmeq4TnxCORgNNnuPTpkfPCItwAoKMQjo7s6+uhfToL4TkE4pLdzAwPCFog8wHw81mp8GCs+VL4nFVrkQCiRejOL0SiyPmFSBQ5vxCJIucXIlHk/EIkSktX+90dzlYjI6uUORKIU4/k9suSwAcAqC/wVdRYsJC1hQMwUOdLwIbIynxXH7WNXHUNtQ0MDFDbwUOHgu1jY2O0T3cv397CLF+ltljwVHd4lT0bUQhKk/wa8FwkoKbK+82VwoFaVed9urv4an9nJIjo9AxPAVecmaY2lu+wWuFBZuUyOa5IurbF6M4vRKLI+YVIFDm/EIki5xciUeT8QiSKnF+IRGmp1FcqlvDG/teDtnwHr4TC8rflSMAPAGQiEtv0LJddPCIftrWFpa1apPrL+x+coLZYIMju62+ktttvv53aqh7+PH//GB9HLFColuOXSKUWybvIgm0iZbzmIzJVpsiDoGI5DaeKc+F9jXNZrp/IlED8Oj05wXMQFiNVgFgVo+oCl/pYkFlNgT1CiKWQ8wuRKHJ+IRJFzi9Eosj5hUgUOb8QibIiqc/MDgOYAVADUHX3kdj7i/PzeG3f/qBt49AG2m/Xrl3B9iwpcwQAlTrPSzd2nEe4zc5yCahWDWt6g4ODfBwVLofNFfkY52a5tFVZ4HLZ7FxYHpqc4scVK4VlOX5/aOvkEXo7NoWrtVuktNmGyNyXy5HyWm18HDUy/1ki2wJAucTnfmE2LB0CwHzkXNcjkZ9Fko9vIRLVx/IWekR2XszF0Pn/2N0nLsJ2hBAtRF/7hUiUlTq/A/iFmb1sZvdejAEJIVrDSr/23+Hux8xsPYCnzOwtd3/u3Dc0PxTuBYBs5DeiEKK1rOjO7+7Hmv+PA/g5gFsC73nA3UfcfSQbScUkhGgtF+z8ZtZlZj1nXwP4UwDhqB0hxGXHSr72DwH4ebP0VRuAv3X3/xXrUK/XUSRJDst9PFFkLhuWZdoi0WiZiORx+jSPvpoY52Wy5ufng+3rB9fTPuvXcwkz18ETReYi0WMxqW+GlACrRpJcdvVwqW8BF1aK7NZbPxZsj0YQEikVAI59cJzaenp46a3Z6XAE5/wZHmX3zltvU9vEKI+O9IjOlovI0siG78F1Eu0HAO0kkjFWim4xF+z87n4IwA0X2l8IsbpI6hMiUeT8QiSKnF+IRJHzC5Eocn4hEqWlCTwBIGvhzxuLSHMZIl9kwWWNtgx/mjAT6Xc+UslZFhYikXtzPArsTERyLBe5xNYeqXfH5qQYSYBZnAtLmAAwUz1DbZm2yL2DaK0lUmMOADKR+oqVKu/X17eZ2gqdYcm01M3lwXfe/jW1TZ3h8xGLwuvv6aa29vawDBhLJlsohJOMsmi/4HuX/U4hxG8Vcn4hEkXOL0SiyPmFSBQ5vxCJ0trV/rqjWg7nKytFVpwLJMilRIJYAGChzG1sewBQX+B59dhKem+kvNPBQ+9RWyx33mv7fkVtjz72CLXVSO2wa0geRACYnuO58zZfuZHabryRlxSr1cLlpPr6+Sr7XOQa6MzznHunp3gwFjtnHVl+6Xf3FKit0MWvHV5srBHUxmBzNTAwQPssVLgatFx05xciUeT8QiSKnF+IRJHzC5Eocn4hEkXOL0SitFTqs0wG+Xw4b91AJB+c18NSSHchItlVufBSyPN8aplIhNH06clg+/wczwc3NMiP69B7R6jt6t/h0twn/+gPqO34WLgU2XvvHaJ9rr3uOmrr7uMy5vzsFLUdfPutYHs+z7fX3c2DX9auXUdtC6TcFQBkyCV+5fYraJ+BSBDOkXd4fr+tO/g2x4/z3H9MXj5x/Cjts37dWmpbLrrzC5Eocn4hEkXOL0SiyPmFSBQ5vxCJIucXIlGWlPrM7EEAnwEw7u7XNtsGAPwEwDYAhwF83t15QromhUIeN1x/fdC2YQMvazUzFc6b1pHjkV5zM+EyTQCweeMwtfVHIvQmJiaC7Z1tPO/fHXfeSW17XnqJ2mYikXbleX5s1+66Ktg+tLaP9vnUpz5FbX/3P39CbVdt5RF/GOwPNj/99P+mXa7Yuo3atvwBL4nmkVJYx4+TMl9XbKV9tm/bQm2lOT73Z8Bl4kI7L1PWNxiWMScneLRipRiOgHTn0YOLWc6d/68B3LWo7X4Az7j7TgDPNP8WQnyEWNL53f05AIufbrkbwEPN1w8B+OxFHpcQ4hJzob/5h9z97CNLo2hU7BVCfIRY8YKfN+oS0x87Znavme01s72VIs9rLoRoLRfq/GNmNgwAzf/H2Rvd/QF3H3H3kfbIM/VCiNZyoc7/BIB7mq/vAfD4xRmOEKJVLEfqewTAnQAGzewogG8C+DaAx8zsywCOAPj8cnZWr9dRKoUTa2acyyRHT4TlmuGIPFiNJOLcsI7LRoUOPiXlUrj01hWbuHS496UXqG3y5Elq275jG7Vls1w2as+ES4et6eGyaHGWS0qfuONj1LawwMtJDWwOn5stG3g02pYN/Lw898xT1BYridZdCEfoZSIJMG+/5TZq++xn/ozaSjN8HGOjPKpvikh6V23jciRL/jr6Pj+Xi1nS+d39i8T0yWXvRQhx2aEn/IRIFDm/EIki5xciUeT8QiSKnF+IRGlpAk+vOyrFsMRy7Ngx2u/5Xz4bbM9Hau7NnuHRV793M68x11PgUX0lkqjzyGH+5OKrr/Kaexs38qi48mw4Kg4AiiUuU02Oh2VRi0QevlIMR00CwO1/9HFqGx+nz3ahQqS0oX4eXVjI8THm2yKJVYtcYmvrDCeMXdPF6/HNTfMA1TvvuJ3avMqlzyf/8QlqO/F+uJ7jaDESQXgqfO1Xyst/ilZ3fiESRc4vRKLI+YVIFDm/EIki5xciUeT8QiRKa2v1GZAlklNXd1iSAYDrSdLPtQNcDotJfdfsuJLapqd4/TnUwxFzs2d4n5tv+F1q27XramqbmOAyWixHY39/ONrLLCKjdXHJ9PW9e6ht82YedVYuhxNM9nbkaJ+pUZJsE8CtN3F5NpPhl3FHWziaMVYX0Mh5BuIJPAudfB77uvj1fd9XvxJs7y7wPu+//36w/eDb4VqNIXTnFyJR5PxCJIqcX4hEkfMLkShyfiESpaWr/dlsFmvWrKE2xuDgQLhPhn925Tt4zrp8nq/K1iqRFdvucCBOT1e4RBYAzM2Hg4EAYE1/D7VtGAof81IMbQiXfjpCVocBIF/gWZU7cnw+YrnzukjQ1e6dO2ifSVKWDQAmx7gSsPPqXdQ2OBCej5hCMHac59vbf+AtapuMKDQvv7SX2uZnw2rR5/713bTP8FD4uHI5rqYsRnd+IRJFzi9Eosj5hUgUOb8QiSLnFyJR5PxCJMpyynU9COAzAMbd/dpm27cAfAXA2XpT33D3J5exLWQi8hxjaChcAbxe4znTim380HJZbisWw+XEAKCzPdyvPVLia+2aTdRWiATU7N//KrXt2fP/qO2+++4Ltl9zNZfYypG8b+1ZLh0dnuFBLuvWhIOuDr9/lPa5ZicPdPrVK3w+Nq4Py14A8Arpt+ua3bTP3z78Y2r7v89zqW+QVyLDlduuoLann3462P7x226lfbq6wrkmI/Fbv8FyPPGvAdwVaP+eu9/Y/Lek4wshLi+WdH53fw7AZAvGIoRoISv5zf81M9tvZg+aWfixPSHEZcuFOv8PAOwAcCOAEwC+w95oZvea2V4z21suLj+nuBDi0nJBzu/uY+5ec/c6gB8CuCXy3gfcfcTdRzry/BlyIURruSDnN7Phc/78HIDXL85whBCtYjlS3yMA7gQwaGZHAXwTwJ1mdiMAB3AYwFeXszOrV9BRCkdnDfWF5TwAyGXDkt48yRMHAEPDvCzU3BwvDbbrGi4bMU6dOklta9dt5uMo83XUl/e9wPd3hke/jU58EGyv1HkEXqHAS1eNjlWo7cVXXqO2kZvD3/K6OsM5BgFgz7MvUtvMmVlqe+bxsFQGACVS2qytxO97+17kct72rXz8CzU+V+OR3JDVXHgsYyV+znK18HFVIvL3YpZ0fnf/YqD5R8vegxDiskRP+AmRKHJ+IRJFzi9Eosj5hUgUOb8QidLSBJ7uQLlcDdomJk7TfrOz4eixWDTapk08mm5mhifVnJ/n8iFLMtrRwR9e2r/vTWobjSSl3LaNlxTbtImXk5qeDo9/djYsDQHAEEkGCQADa7hU+Zk//1fUVq96sP3IoSO0T18ff0p844Yt1LZlIz/XJ09OBNtffZVHCcZyYBby4Wg6AChVeBLaGniNteHh8PhZslsAqFbDfnQ+UbO68wuRKHJ+IRJFzi9Eosj5hUgUOb8QiSLnFyJRWir1wQzZtnDSypmIFHX0aLgGWjbLsxXuuIrXb7MMr+MXi3Dr7Az3m4pEbG3Zup3aunt5hNiOHTzh5rFjPCqRST0nTvD6c4fe43X8YDzCrc34vaO2EJa2ro7U1evu7qa2Hz/Ek2pWF7j0uWVzOHHm0aM8keg8D6ajUYIAMFfhtoUaH+Pu68JSa3c3vz6YzC2pTwixJHJ+IRJFzi9Eosj5hUgUOb8QidLS1f72XAe2bgmvYo+O8iCXei28ut2e4+Wuent47aRKB8+1lsvx4IzOzvD+KuVwEAsA9PcPUtvCAu9XnOe52Aw8kGjHlVcF2+dmeRDU8WNhNQUA8oU8tWWI+gEAEySv4VuP/xPtU+jg+5qe5jn8tm7dRm3Dm8KBSQfe/jUfRze/J9adXx/ZSGmz8gK/5jo7wsFCszM8yGyBKBz1Og8gWozu/EIkipxfiESR8wuRKHJ+IRJFzi9Eosj5hUiU5ZTr2gLgbwAMoVGe6wF3/76ZDQD4CYBtaJTs+ry780R8aOTwKxFZ7Mw0l0Jm58P5yrIRWa7mXHYxkosPAHIRuSnTFu7X288lr9hxFbp4jrYFkgMPAGZm+TanzhSD7ZUFfsw33Xw7td1w/Qi15fN8riokAObh//EI7TN6fIzarrv+ZmqL5WT8h5//Y7B97VouBXdwBRm1Gj8vmQx3p3qkitbx46PB9uefe572YdLhzAyXRBeznDt/FcBfuPtuALcBuM/MdgO4H8Az7r4TwDPNv4UQHxGWdH53P+HurzRfzwA4AGATgLsBPNR820MAPnupBimEuPic129+M9sG4CYAewAMufvZIPFRNH4WCCE+Iizb+c2sG8BPAXzd3T+USN/dHY31gFC/e81sr5ntLc7zZAdCiNayLOc3sxwajv+wu/+s2TxmZsNN+zCA4APi7v6Au4+4+0i+EFlJEUK0lCWd38wMwI8AHHD3755jegLAPc3X9wB4/OIPTwhxqVhOVN/HAXwJwGtmdrbG0TcAfBvAY2b2ZQBHAHx+qQ0ViyW8/lo4murIEV7GaXQ0HHV2po9HPb28dz+11Wpcd+nv76e2ajUsr7AIKwBob+cy4A03XHdB/dYObKC2ei0s6Y2PcRV28hSXyjrzfD4OvMHz+/X19QXbZ2bCUiQA5As91NbVw/PZlYp8/jMk0q5Y4nJppRKWloG4TBz7Zlso8PyEkycng+37iq/RPiyHX3Gez+9ilnR+d38eAMuU+cll70kIcVmhJ/yESBQ5vxCJIucXIlHk/EIkipxfiERpaQLPhWoNJ4msMTHBpahiMSy9lEq8TNYvn3uBb+8Ml3nyfXxKimdIdCEPboPzIDDs3/cmtQ0O8sSft956K7W1kXJolQpP7Pjss89S23PP8nmcm+N1rVjUXKXIE4muX88lzHw+nOQSAHKdkUjMXFgyrUdKjbXnuWTXE5Ec29r4NnsjpdnqCEvPsXJong1fp8Yr2P0GuvMLkShyfiESRc4vRKLI+YVIFDm/EIki5xciUcxjWtRFprPQ6VuvviJoGxvjyRtzuXBkVibDP7sqFS7nTU9ziXDjxo3UNjoaTrTIxgcAMC6xZSMRYvORxCcbNnAZkJ3PnTt30j4x3nzzDWorlyNJV6fDkh7JgQoA6OnhkW9TU7FafeF6fABQnAtHfsauj1hEZXd3gdrm5/kYe7p4v1otHJXY3cP7sJp8B98aRXG+sizBT3d+IRJFzi9Eosj5hUgUOb8QiSLnFyJRWhrYU6vVMDM1HbS1k1xrAFAlOdWqEaUipgT0dvEgi9ORAKOMh7e5OaIQxMpCMfUAAHZ/fDe1HT586Ly3WZ7nATWFAl9V3jy8idqqVZ7r7vTp8Dy2t/OgmYmJCT6OzXyOS6Q0GMDHGLs+htevo7aR3+dlwx5//B+obbbO8wyygKDaAndPliMxpiAtRnd+IRJFzi9Eosj5hUgUOb8QiSLnFyJR5PxCJMqSUp+ZbQHwN2iU4HYAD7j7983sWwC+AuBk863fcPcnY9uq1+qYOROW+tra+FBYoEWsTyxgqVLhslc1EudkxHbmNA8U2n7FNmrzdTzoZ3rqDLXt2H4ltbW3hSXTaoVLTUXnZc/asvz+kO/kefW6C2FbWzuXdHt6eLmu9YNcfouda1Z+bfv27bTP7910A7Wti0i3bx14ndoOHXyH2gZ6w7JdJsPjc8ZPHA22Vxd4wNJilqPzVwH8hbu/YmY9AF42s6eatu+5+39Z9t6EEJcNy6nVdwLAiebrGTM7AIA/+SGE+EhwXr/5zWwbgJsA7Gk2fc3M9pvZg2a25iKPTQhxCVm285tZN4CfAvi6u08D+AGAHQBuROObwXdIv3vNbK+Z7fV66xKHCCHiLMv5zSyHhuM/7O4/AwB3H3P3mrvXAfwQwC2hvu7+gLuPuPuIRRYwhBCtZUnnNzMD8CMAB9z9u+e0D5/zts8B4EudQojLjuWs9n8cwJcAvGZmrzbbvgHgi2Z2Ixry32EAX11qQ7lcGzYNh0syxeQaFpkVy8NWKvNIr1qV94tUXEJ7viPYXp4v0j6TJ8epbc0avkxy/Phxast38MitjlzYxqLsAODIkWPUFktPGMtnNzAQzjNYQ0SCLXM5Mt/B8+rFItl2bA/njCzOhSVnANj7Ei9RVq/yMS4UeQ6//kg+vn4yj8USL4e2aV1Y+hw9wc/lYpaz2v88gND39aimL4S4vNETfkIkipxfiESR8wuRKHJ+IRJFzi9EorQ0gSfcqVQSk+2KxbCUVirzaLQKSfrZHMYFYRfwjNLpSZ6U8mO3/T61xcpavfACl6I6O8MJMgcGBmifQie/DHLt/KAjAX/IWC3YfnKcz8fcHI+2nJ46RW2sdBUAEOUTpSKX0Y4f+4DaFiLJQtcN8jnubOPzOD8bjgo9fZof8+/+zq5gezZ2UhahO78QiSLnFyJR5PxCJIqcX4hEkfMLkShyfiESpbW1+qpVnJkMR7nFovpY1Naanm7ap7MzHIG31L7m57l8WCTyUG8vr/1XK/OIvyPvHuT7IvJPw8ZlzA3r8sH2jPNotIE+noizo4NfImExr9mvPdyvPcclL+viIYTd3fxc5/PhYwaAQj68zQ3rt9A+u3Zuo7bZaZ5Y1SPJMz94/zC1ZerhmczW+HU6czosmdYj9RN/Y7/LfqcQ4rcKOb8QiSLnFyJR5PxCJIqcX4hEkfMLkSgtlfra2rJYOxCuS8aSdMaISTyxum85FuqFC4su7OzkySUtEgr47sG3LmgcW7dwaXFofbiW3JEjR3ifoSFqi81V7JzlSMLNtQM8aWkuki10/fr11LbzmqupzWvhiL/Dh7jM2tkfvkYBoC3DIwg7usIRlQBwzQ5eG3B4KHzOWG1IABhYE65B+PYHT/NOi9CdX4hEkfMLkShyfiESRc4vRKLI+YVIlCVX+82sE8BzADqa7/97d/+mmW0H8CiAtQBeBvAld+dL1AAyZiiQVeAiCW4A+Cr7TIUH4SyUeY62jg4e9JPJRD4PPbzSW5rnQTM7d+6ktm1bN1HbqVM8f1tslb1SCefB27ZlM+1zyy3BGqsA4nkSi2Wec29uLjz/3d28TFbsuGKlwa66kq+kv/3rA8H2a68N58ADgHKJH/P4cX5d5Tu4WtGe565WKob3VybtAFAhpcEWIirRYpZz5y8D+IS734BGOe67zOw2AH8F4HvufhWA0wC+vOy9CiFWnSWd3xuc/ZjJNf85gE8A+Ptm+0MAPntJRiiEuCQs6ze/mWWbFXrHATwF4F0AU+5+9nvaUQD8O6wQ4rJjWc7v7jV3vxHAZgC3AOA/mBZhZvea2V4z21slT1sJIVrPea32u/sUgH8B8DEA/WZ2dhVjM4BgYXB3f8DdR9x9pO08CgoIIS4tS3qjma0zs/7m6zyAPwFwAI0PgX/TfNs9AB6/VIMUQlx8lhPYMwzgITPLovFh8Zi7/5OZvQngUTP7TwB+BeBHS22oVq9hZjacAy1WcilLSh1lMlxaaaxJhmFy2FLjYLaYDPXuu+9QWyygJhMp17V+7SA3EjZv5lJf3bnEVorIXrUqnyuWC7EWyXNXjpyXWp3n8FtDgsUAwKthCfnY0aO0z0AfD5zq7eXjsFok0CkbKXtGrtW+Ll7+K5cNuy7LdxliSed39/0Abgq0H0Lj978Q4iOIfoQLkShyfiESRc4vRKLI+YVIFDm/EIlisdJVF31nZicBnE0mNwggXHOotWgcH0bj+DAftXFc4e7rlrPBljr/h3ZsttfdR1Zl5xqHxqFx6Gu/EKki5xciUVbT+R9YxX2fi8bxYTSOD/NbO45V+80vhFhd9LVfiERZFec3s7vM7G0zO2hm96/GGJrjOGxmr5nZq2a2t4X7fdDMxs3s9XPaBszsKTN7p/k/r2t1acfxLTM71pyTV83s0y0YxxYz+xcze9PM3jCzf9dsb+mcRMbR0jkxs04ze9HM9jXH8R+b7dvNbE/Tb35iZrxO3HJw95b+A5BFIw3YlQDaAewDsLvV42iO5TCAwVXY7x8CuBnA6+e0/WcA9zdf3w/gr1ZpHN8C8O9bPB/DAG5uvu4B8GsAu1s9J5FxtHROABiA7ubrHIA9AG4D8BiALzTb/xuAf7uS/azGnf8WAAfd/ZA3Un0/CuDuVRjHquHuzwGYXNR8NxqJUIEWJUQl42g57n7C3V9pvp5BI1nMJrR4TiLjaCne4JInzV0N598E4INz/l7N5J8O4Bdm9rKZ3btKYzjLkLufaL4eBcAzfVx6vmZm+5s/Cy75z49zMbNtaOSP2INVnJNF4wBaPCetSJqb+oLfHe5+M4A/A3Cfmf3hag8IaHzyI5aK6NLyAwA70KjRcALAd1q1YzPrBvBTAF939w9V92jlnATG0fI58RUkzV0uq+H8xwBsOedvmvzzUuPux5r/jwP4OVY3M9GYmQ0DQPP/8dUYhLuPNS+8OoAfokVzYmY5NBzuYXf/WbO55XMSGsdqzUlz3+edNHe5rIbzvwRgZ3Plsh3AFwA80epBmFmXmfWcfQ3gTwG8Hu91SXkCjUSowComRD3rbE0+hxbMiZkZGjkgD7j7d88xtXRO2DhaPSctS5rbqhXMRauZn0ZjJfVdAP9hlcZwJRpKwz4Ab7RyHAAeQePr4wIav92+jEbNw2cAvAPgaQADqzSOHwN4DcB+NJxvuAXjuAONr/T7Abza/PfpVs9JZBwtnRMA16ORFHc/Gh80f3nONfsigIMA/g5Ax0r2oyf8hEiU1Bf8hEgWOb8QiSLnFyJR5PxCJIqcX4hEkfMLkShyfiESRc4vRKL8f/ifjT2T45lEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704    barbershop\n",
      "Name: description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "plotData = train_images[0]\n",
    "plt.imshow(plotData)\n",
    "plt.show()\n",
    "print(text_labels.loc[text_labels['label']==train_labels[0], 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547    cash_machine\n",
      "Name: description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "plotData = val_images[0]\n",
    "plt.imshow(plotData)\n",
    "plt.show()\n",
    "print(text_labels.loc[text_labels['label']==val_labels[0], 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32')\n",
    "val_images = val_images.astype('float32')\n",
    "train_images /= 255\n",
    "val_images /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "y_test = []\n",
    "for row in train_labels:\n",
    "    label = text_labels.loc[text_labels['label']==row]['id'].iloc[0]\n",
    "    y_train.append(label)\n",
    "\n",
    "for row in val_labels:\n",
    "    label = text_labels.loc[text_labels['label']==row]['id'].iloc[0]\n",
    "    y_test.append(label)\n",
    "    \n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = OneHotEncoder(categories='auto')\n",
    "y_train = enc.fit_transform(train_labels.reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(val_labels.reshape(-1, 1)).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['n01443537', 'n01629819', 'n01641577', 'n01644900', 'n01698640',\n",
       "        'n01742172', 'n01768244', 'n01770393', 'n01774384', 'n01774750',\n",
       "        'n01784675', 'n01855672', 'n01882714', 'n01910747', 'n01917289',\n",
       "        'n01944390', 'n01945685', 'n01950731', 'n01983481', 'n01984695',\n",
       "        'n02002724', 'n02056570', 'n02058221', 'n02074367', 'n02085620',\n",
       "        'n02094433', 'n02099601', 'n02099712', 'n02106662', 'n02113799',\n",
       "        'n02123045', 'n02123394', 'n02124075', 'n02125311', 'n02129165',\n",
       "        'n02132136', 'n02165456', 'n02190166', 'n02206856', 'n02226429',\n",
       "        'n02231487', 'n02233338', 'n02236044', 'n02268443', 'n02279972',\n",
       "        'n02281406', 'n02321529', 'n02364673', 'n02395406', 'n02403003',\n",
       "        'n02410509', 'n02415577', 'n02423022', 'n02437312', 'n02480495',\n",
       "        'n02481823', 'n02486410', 'n02504458', 'n02509815', 'n02666196',\n",
       "        'n02669723', 'n02699494', 'n02730930', 'n02769748', 'n02788148',\n",
       "        'n02791270', 'n02793495', 'n02795169', 'n02802426', 'n02808440',\n",
       "        'n02814533', 'n02814860', 'n02815834', 'n02823428', 'n02837789',\n",
       "        'n02841315', 'n02843684', 'n02883205', 'n02892201', 'n02906734',\n",
       "        'n02909870', 'n02917067', 'n02927161', 'n02948072', 'n02950826',\n",
       "        'n02963159', 'n02977058', 'n02988304', 'n02999410', 'n03014705',\n",
       "        'n03026506', 'n03042490', 'n03085013', 'n03089624', 'n03100240',\n",
       "        'n03126707', 'n03160309', 'n03179701', 'n03201208', 'n03250847',\n",
       "        'n03255030', 'n03355925', 'n03388043', 'n03393912', 'n03400231',\n",
       "        'n03404251', 'n03424325', 'n03444034', 'n03447447', 'n03544143',\n",
       "        'n03584254', 'n03599486', 'n03617480', 'n03637318', 'n03649909',\n",
       "        'n03662601', 'n03670208', 'n03706229', 'n03733131', 'n03763968',\n",
       "        'n03770439', 'n03796401', 'n03804744', 'n03814639', 'n03837869',\n",
       "        'n03838899', 'n03854065', 'n03891332', 'n03902125', 'n03930313',\n",
       "        'n03937543', 'n03970156', 'n03976657', 'n03977966', 'n03980874',\n",
       "        'n03983396', 'n03992509', 'n04008634', 'n04023962', 'n04067472',\n",
       "        'n04070727', 'n04074963', 'n04099969', 'n04118538', 'n04133789',\n",
       "        'n04146614', 'n04149813', 'n04179913', 'n04251144', 'n04254777',\n",
       "        'n04259630', 'n04265275', 'n04275548', 'n04285008', 'n04311004',\n",
       "        'n04328186', 'n04356056', 'n04366367', 'n04371430', 'n04376876',\n",
       "        'n04398044', 'n04399382', 'n04417672', 'n04456115', 'n04465501',\n",
       "        'n04486054', 'n04487081', 'n04501370', 'n04507155', 'n04532106',\n",
       "        'n04532670', 'n04540053', 'n04560804', 'n04562935', 'n04596742',\n",
       "        'n04597913', 'n06596364', 'n07579787', 'n07583066', 'n07614500',\n",
       "        'n07615774', 'n07695742', 'n07711569', 'n07715103', 'n07720875',\n",
       "        'n07734744', 'n07747607', 'n07749582', 'n07753592', 'n07768694',\n",
       "        'n07871810', 'n07873807', 'n07875152', 'n07920052', 'n09193705',\n",
       "        'n09246464', 'n09256479', 'n09332890', 'n09428293', 'n12267677'],\n",
       "       dtype='<U9')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(y_train, open( \"../imagenet-200/y_train.pkl\", \"wb\" ) )\n",
    "pickle.dump(y_test, open( \"../imagenet-200/y_test.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(enc, open( \"../imagenet-200/encoder.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, y_train = shuffle(train_images, train_labels, y_train, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label   id description\n",
      "812  n07583066  813   guacamole\n"
     ]
    }
   ],
   "source": [
    "plotData = train_images[0]\n",
    "plt.imshow(plotData)\n",
    "plt.show()\n",
    "print(text_labels.loc[text_labels['label']==train_labels[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Keras CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "First we will select an optimizer for the task. We will start with a smaller number of epochs to eliminate any that are obviously poor choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD optimizer\n",
      "Epoch 1/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.2982 - acc: 0.0052 - val_loss: 5.2933 - val_acc: 0.0059\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.2358 - acc: 0.0096 - val_loss: 5.1405 - val_acc: 0.0113\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.1478 - acc: 0.0128 - val_loss: 5.0788 - val_acc: 0.0182\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.0939 - acc: 0.0174 - val_loss: 4.9952 - val_acc: 0.0296\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.0258 - acc: 0.0234 - val_loss: 4.8967 - val_acc: 0.0385\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9411 - acc: 0.0317 - val_loss: 4.7850 - val_acc: 0.0496\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8614 - acc: 0.0387 - val_loss: 4.7128 - val_acc: 0.0599\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7902 - acc: 0.0462 - val_loss: 4.6497 - val_acc: 0.0657\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7283 - acc: 0.0521 - val_loss: 4.6031 - val_acc: 0.0665\n",
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6744 - acc: 0.0587 - val_loss: 4.5275 - val_acc: 0.0788\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6280 - acc: 0.0647 - val_loss: 4.4615 - val_acc: 0.0880\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5830 - acc: 0.0706 - val_loss: 4.4305 - val_acc: 0.0939\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5409 - acc: 0.0760 - val_loss: 4.3853 - val_acc: 0.0943\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5050 - acc: 0.0816 - val_loss: 4.3523 - val_acc: 0.1005\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4708 - acc: 0.0868 - val_loss: 4.3456 - val_acc: 0.0964\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4376 - acc: 0.0893 - val_loss: 4.2849 - val_acc: 0.1062\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4078 - acc: 0.0931 - val_loss: 4.2400 - val_acc: 0.1165\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3811 - acc: 0.0969 - val_loss: 4.2200 - val_acc: 0.1150\n",
      "Epoch 19/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3562 - acc: 0.1006 - val_loss: 4.2318 - val_acc: 0.1136\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3361 - acc: 0.1032 - val_loss: 4.1650 - val_acc: 0.1234\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3074 - acc: 0.1067 - val_loss: 4.1405 - val_acc: 0.1278\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2864 - acc: 0.1098 - val_loss: 4.1719 - val_acc: 0.1237\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2615 - acc: 0.1119 - val_loss: 4.1302 - val_acc: 0.1299\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2445 - acc: 0.1147 - val_loss: 4.2019 - val_acc: 0.1206\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2264 - acc: 0.1174 - val_loss: 4.1289 - val_acc: 0.1295\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2088 - acc: 0.1194 - val_loss: 4.1744 - val_acc: 0.1202\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1874 - acc: 0.1221 - val_loss: 4.1749 - val_acc: 0.1229\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1771 - acc: 0.1242 - val_loss: 4.0753 - val_acc: 0.1412\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 24s 8ms/step - loss: 4.1562 - acc: 0.1259 - val_loss: 4.1757 - val_acc: 0.1222\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1403 - acc: 0.1293 - val_loss: 4.0168 - val_acc: 0.1463\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1272 - acc: 0.1289 - val_loss: 4.1705 - val_acc: 0.1198\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1112 - acc: 0.1322 - val_loss: 4.0494 - val_acc: 0.1385\n",
      "Epoch 33/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1021 - acc: 0.1346 - val_loss: 4.0837 - val_acc: 0.1357\n",
      "Epoch 34/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0830 - acc: 0.1359 - val_loss: 4.0721 - val_acc: 0.1364\n",
      "Epoch 35/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0707 - acc: 0.1379 - val_loss: 4.0735 - val_acc: 0.1348\n",
      "Epoch 36/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0609 - acc: 0.1389 - val_loss: 4.0804 - val_acc: 0.1352\n",
      "Epoch 37/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0474 - acc: 0.1418 - val_loss: 4.0743 - val_acc: 0.1349\n",
      "Epoch 38/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0341 - acc: 0.1437 - val_loss: 4.0432 - val_acc: 0.1412\n",
      "Epoch 39/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0242 - acc: 0.1435 - val_loss: 4.0185 - val_acc: 0.1446\n",
      "Epoch 40/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0090 - acc: 0.1469 - val_loss: 4.0403 - val_acc: 0.1381\n",
      "Epoch 41/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9978 - acc: 0.1466 - val_loss: 4.1210 - val_acc: 0.1287\n",
      "Epoch 42/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9854 - acc: 0.1488 - val_loss: 4.2714 - val_acc: 0.1142\n",
      "Epoch 43/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9778 - acc: 0.1514 - val_loss: 4.1216 - val_acc: 0.1333\n",
      "Epoch 44/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9667 - acc: 0.1522 - val_loss: 4.0706 - val_acc: 0.1378\n",
      "Epoch 45/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9570 - acc: 0.1543 - val_loss: 4.0405 - val_acc: 0.1451\n",
      "Epoch 46/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9447 - acc: 0.1545 - val_loss: 3.9572 - val_acc: 0.1541\n",
      "Epoch 47/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9310 - acc: 0.1562 - val_loss: 3.9463 - val_acc: 0.1540\n",
      "Epoch 48/50\n",
      "3125/3125 [==============================] - 24s 8ms/step - loss: 3.9220 - acc: 0.1574 - val_loss: 3.9665 - val_acc: 0.1487\n",
      "Epoch 49/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9109 - acc: 0.1587 - val_loss: 3.9192 - val_acc: 0.1588\n",
      "Epoch 50/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9045 - acc: 0.1610 - val_loss: 3.8916 - val_acc: 0.1597\n",
      "10000/10000 [==============================] - 1s 143us/step\n",
      "Test loss: 3.8916304641723634\n",
      "Test accuracy: 0.1597\n",
      "Runtime: 1234.9703407287598\n",
      "Training RMSprop optimizer\n",
      "Epoch 1/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9497 - acc: 0.0327 - val_loss: 4.6417 - val_acc: 0.0590\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7054 - acc: 0.0578 - val_loss: 4.5453 - val_acc: 0.0767\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6665 - acc: 0.0653 - val_loss: 4.5877 - val_acc: 0.0719\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6640 - acc: 0.0648 - val_loss: 4.6474 - val_acc: 0.0651\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6786 - acc: 0.0628 - val_loss: 4.7051 - val_acc: 0.0626\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6987 - acc: 0.0622 - val_loss: 4.7183 - val_acc: 0.0624\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7219 - acc: 0.0590 - val_loss: 4.5778 - val_acc: 0.0723\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7441 - acc: 0.0578 - val_loss: 4.6507 - val_acc: 0.0646\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7820 - acc: 0.0527 - val_loss: 4.6735 - val_acc: 0.0586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8314 - acc: 0.0474 - val_loss: 4.7025 - val_acc: 0.0573\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8661 - acc: 0.0446 - val_loss: 4.7488 - val_acc: 0.0515\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8788 - acc: 0.0425 - val_loss: 4.7716 - val_acc: 0.0485\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8849 - acc: 0.0404 - val_loss: 4.9086 - val_acc: 0.0411\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8923 - acc: 0.0406 - val_loss: 5.0777 - val_acc: 0.0388\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8887 - acc: 0.0394 - val_loss: 4.7517 - val_acc: 0.0524\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8673 - acc: 0.0411 - val_loss: 4.7493 - val_acc: 0.0458\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8601 - acc: 0.0401 - val_loss: 4.7097 - val_acc: 0.0535\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8470 - acc: 0.0425 - val_loss: 4.7704 - val_acc: 0.0481\n",
      "Epoch 19/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8352 - acc: 0.0435 - val_loss: 4.8183 - val_acc: 0.0442\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8278 - acc: 0.0458 - val_loss: 4.8052 - val_acc: 0.0421\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8295 - acc: 0.0450 - val_loss: 4.7031 - val_acc: 0.0524\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8186 - acc: 0.0454 - val_loss: 4.7653 - val_acc: 0.0523\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8247 - acc: 0.0443 - val_loss: 4.7247 - val_acc: 0.0496\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8234 - acc: 0.0449 - val_loss: 4.7317 - val_acc: 0.0519\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8266 - acc: 0.0456 - val_loss: 4.7167 - val_acc: 0.0544\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8279 - acc: 0.0433 - val_loss: 4.7490 - val_acc: 0.0511\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8319 - acc: 0.0445 - val_loss: 4.7456 - val_acc: 0.0505\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8407 - acc: 0.0439 - val_loss: 4.8018 - val_acc: 0.0413\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8390 - acc: 0.0435 - val_loss: 4.7389 - val_acc: 0.0494\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8412 - acc: 0.0437 - val_loss: 4.8182 - val_acc: 0.0416\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8499 - acc: 0.0424 - val_loss: 4.8440 - val_acc: 0.0449\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8590 - acc: 0.0415 - val_loss: 4.9603 - val_acc: 0.0354\n",
      "Epoch 33/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8584 - acc: 0.0407 - val_loss: 4.8370 - val_acc: 0.0374\n",
      "Epoch 34/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8610 - acc: 0.0410 - val_loss: 4.8089 - val_acc: 0.0377\n",
      "Epoch 35/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8612 - acc: 0.0407 - val_loss: 4.8935 - val_acc: 0.0375\n",
      "Epoch 36/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8641 - acc: 0.0404 - val_loss: 4.8408 - val_acc: 0.0406\n",
      "Epoch 37/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8747 - acc: 0.0399 - val_loss: 4.8694 - val_acc: 0.0340\n",
      "Epoch 38/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8770 - acc: 0.0393 - val_loss: 4.7873 - val_acc: 0.0460\n",
      "Epoch 39/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8864 - acc: 0.0385 - val_loss: 4.8266 - val_acc: 0.0361\n",
      "Epoch 40/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8919 - acc: 0.0376 - val_loss: 4.9038 - val_acc: 0.0336\n",
      "Epoch 41/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8899 - acc: 0.0384 - val_loss: 4.7996 - val_acc: 0.0473\n",
      "Epoch 42/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8984 - acc: 0.0370 - val_loss: 4.8612 - val_acc: 0.0366\n",
      "Epoch 43/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8913 - acc: 0.0380 - val_loss: 4.8058 - val_acc: 0.0419\n",
      "Epoch 44/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8998 - acc: 0.0375 - val_loss: 4.8195 - val_acc: 0.0421\n",
      "Epoch 45/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9006 - acc: 0.0376 - val_loss: 4.9566 - val_acc: 0.0338\n",
      "Epoch 46/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9067 - acc: 0.0368 - val_loss: 4.8658 - val_acc: 0.0320\n",
      "Epoch 47/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9069 - acc: 0.0369 - val_loss: 4.8185 - val_acc: 0.0390\n",
      "Epoch 48/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9061 - acc: 0.0370 - val_loss: 4.8666 - val_acc: 0.0355\n",
      "Epoch 49/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9133 - acc: 0.0364 - val_loss: 4.9605 - val_acc: 0.0291\n",
      "Epoch 50/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9130 - acc: 0.0364 - val_loss: 4.8804 - val_acc: 0.0287\n",
      "10000/10000 [==============================] - 1s 126us/step\n",
      "Test loss: 4.8803506820678715\n",
      "Test accuracy: 0.0287\n",
      "Runtime: 1247.7888658046722\n",
      "Training Adagrad optimizer\n",
      "Epoch 1/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.0430 - acc: 0.0224 - val_loss: 4.7617 - val_acc: 0.0491\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7292 - acc: 0.0516 - val_loss: 4.5426 - val_acc: 0.0745\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5925 - acc: 0.0673 - val_loss: 4.4259 - val_acc: 0.0907\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5171 - acc: 0.0785 - val_loss: 4.3626 - val_acc: 0.0992\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 4.4562 - acc: 0.0866 - val_loss: 4.2970 - val_acc: 0.1065\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4061 - acc: 0.0919 - val_loss: 4.3206 - val_acc: 0.1043\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3694 - acc: 0.0973 - val_loss: 4.3405 - val_acc: 0.1048\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3333 - acc: 0.1035 - val_loss: 4.2853 - val_acc: 0.1080\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3041 - acc: 0.1048 - val_loss: 4.2710 - val_acc: 0.1101\n",
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2809 - acc: 0.1085 - val_loss: 4.2600 - val_acc: 0.1118\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2562 - acc: 0.1125 - val_loss: 4.2412 - val_acc: 0.1150\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2338 - acc: 0.1155 - val_loss: 4.1637 - val_acc: 0.1252\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2137 - acc: 0.1174 - val_loss: 4.1390 - val_acc: 0.1302\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1987 - acc: 0.1204 - val_loss: 4.2319 - val_acc: 0.1172\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1863 - acc: 0.1222 - val_loss: 4.2110 - val_acc: 0.1191\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1675 - acc: 0.1244 - val_loss: 4.2286 - val_acc: 0.1162\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1510 - acc: 0.1258 - val_loss: 4.1580 - val_acc: 0.1250\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1432 - acc: 0.1275 - val_loss: 4.1890 - val_acc: 0.1215\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1299 - acc: 0.1305 - val_loss: 4.1555 - val_acc: 0.1255\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1236 - acc: 0.1308 - val_loss: 4.1778 - val_acc: 0.1226\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1100 - acc: 0.1333 - val_loss: 4.2052 - val_acc: 0.1174\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0985 - acc: 0.1342 - val_loss: 4.1471 - val_acc: 0.1253\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0925 - acc: 0.1353 - val_loss: 4.1496 - val_acc: 0.1262\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0845 - acc: 0.1360 - val_loss: 4.1684 - val_acc: 0.1223\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0747 - acc: 0.1369 - val_loss: 4.1530 - val_acc: 0.1237\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0631 - acc: 0.1391 - val_loss: 4.1565 - val_acc: 0.1246\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0620 - acc: 0.1400 - val_loss: 4.1971 - val_acc: 0.1196\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0483 - acc: 0.1413 - val_loss: 4.1264 - val_acc: 0.1279\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0443 - acc: 0.1422 - val_loss: 4.0598 - val_acc: 0.1377\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0368 - acc: 0.1428 - val_loss: 4.2253 - val_acc: 0.1180\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0305 - acc: 0.1431 - val_loss: 4.1602 - val_acc: 0.1230\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0269 - acc: 0.1440 - val_loss: 4.1068 - val_acc: 0.1300\n",
      "Epoch 33/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0188 - acc: 0.1449 - val_loss: 4.1372 - val_acc: 0.1270\n",
      "Epoch 34/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0123 - acc: 0.1467 - val_loss: 4.1766 - val_acc: 0.1239\n",
      "Epoch 35/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0030 - acc: 0.1482 - val_loss: 4.1373 - val_acc: 0.1261\n",
      "Epoch 36/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9990 - acc: 0.1476 - val_loss: 4.0761 - val_acc: 0.1321\n",
      "Epoch 37/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9918 - acc: 0.1495 - val_loss: 4.1096 - val_acc: 0.1307\n",
      "Epoch 38/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9908 - acc: 0.1497 - val_loss: 4.1536 - val_acc: 0.1266\n",
      "Epoch 39/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9815 - acc: 0.1502 - val_loss: 4.1080 - val_acc: 0.1300\n",
      "Epoch 40/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9758 - acc: 0.1493 - val_loss: 4.1419 - val_acc: 0.1265\n",
      "Epoch 41/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9753 - acc: 0.1492 - val_loss: 4.1194 - val_acc: 0.1298\n",
      "Epoch 42/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9690 - acc: 0.1520 - val_loss: 4.1092 - val_acc: 0.1300\n",
      "Epoch 43/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9658 - acc: 0.1543 - val_loss: 4.1268 - val_acc: 0.1303\n",
      "Epoch 44/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9614 - acc: 0.1527 - val_loss: 4.0905 - val_acc: 0.1331\n",
      "Epoch 45/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9575 - acc: 0.1542 - val_loss: 4.0526 - val_acc: 0.1372\n",
      "Epoch 46/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9534 - acc: 0.1543 - val_loss: 4.1526 - val_acc: 0.1275\n",
      "Epoch 47/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 3.9482 - acc: 0.1560 - val_loss: 4.1177 - val_acc: 0.1327\n",
      "Epoch 48/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9403 - acc: 0.1564 - val_loss: 4.1136 - val_acc: 0.1303\n",
      "Epoch 49/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9391 - acc: 0.1569 - val_loss: 4.1067 - val_acc: 0.1333\n",
      "Epoch 50/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9312 - acc: 0.1576 - val_loss: 4.1348 - val_acc: 0.1302\n",
      "10000/10000 [==============================] - 1s 127us/step\n",
      "Test loss: 4.134802265167236\n",
      "Test accuracy: 0.1302\n",
      "Runtime: 1253.2157349586487\n",
      "Training Adadelta optimizer\n",
      "Epoch 1/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.0688 - acc: 0.0197 - val_loss: 4.8003 - val_acc: 0.0404\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7374 - acc: 0.0538 - val_loss: 4.5508 - val_acc: 0.0770\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5863 - acc: 0.0702 - val_loss: 4.5602 - val_acc: 0.0775\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5066 - acc: 0.0820 - val_loss: 4.3407 - val_acc: 0.1019\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4519 - acc: 0.0892 - val_loss: 4.2902 - val_acc: 0.1038\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4108 - acc: 0.0955 - val_loss: 4.2413 - val_acc: 0.1149\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3816 - acc: 0.0987 - val_loss: 4.2411 - val_acc: 0.1129\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3686 - acc: 0.1008 - val_loss: 4.3086 - val_acc: 0.1061\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3590 - acc: 0.1033 - val_loss: 4.1823 - val_acc: 0.1233\n",
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3488 - acc: 0.1061 - val_loss: 4.2585 - val_acc: 0.1155\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3364 - acc: 0.1084 - val_loss: 4.3537 - val_acc: 0.1040\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3423 - acc: 0.1066 - val_loss: 4.3385 - val_acc: 0.1022\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3320 - acc: 0.1072 - val_loss: 4.2066 - val_acc: 0.1194\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3307 - acc: 0.1083 - val_loss: 4.2385 - val_acc: 0.1162\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3284 - acc: 0.1088 - val_loss: 4.3756 - val_acc: 0.0983\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3311 - acc: 0.1092 - val_loss: 4.3474 - val_acc: 0.1034\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3360 - acc: 0.1089 - val_loss: 4.4212 - val_acc: 0.0930\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3363 - acc: 0.1092 - val_loss: 4.3415 - val_acc: 0.1086\n",
      "Epoch 19/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3319 - acc: 0.1097 - val_loss: 4.3229 - val_acc: 0.1029\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3295 - acc: 0.1077 - val_loss: 4.3954 - val_acc: 0.0960\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3324 - acc: 0.1092 - val_loss: 4.2541 - val_acc: 0.1120\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3340 - acc: 0.1097 - val_loss: 4.4092 - val_acc: 0.0975\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3355 - acc: 0.1098 - val_loss: 4.2614 - val_acc: 0.1139\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3351 - acc: 0.1095 - val_loss: 4.1202 - val_acc: 0.1311\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3396 - acc: 0.1092 - val_loss: 4.4107 - val_acc: 0.0954\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3356 - acc: 0.1088 - val_loss: 4.4070 - val_acc: 0.0974\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3386 - acc: 0.1085 - val_loss: 4.4761 - val_acc: 0.0885\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3387 - acc: 0.1086 - val_loss: 4.2466 - val_acc: 0.1161\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3402 - acc: 0.1079 - val_loss: 4.3862 - val_acc: 0.1019\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3423 - acc: 0.1081 - val_loss: 4.2275 - val_acc: 0.1104\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3452 - acc: 0.1086 - val_loss: 4.4524 - val_acc: 0.0967\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3498 - acc: 0.1073 - val_loss: 4.4561 - val_acc: 0.0919\n",
      "Epoch 33/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3468 - acc: 0.1082 - val_loss: 4.4930 - val_acc: 0.0921\n",
      "Epoch 34/50\n",
      "3125/3125 [==============================] - 27s 9ms/step - loss: 4.3447 - acc: 0.1069 - val_loss: 4.2863 - val_acc: 0.1085\n",
      "Epoch 35/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3521 - acc: 0.1067 - val_loss: 4.4492 - val_acc: 0.0914\n",
      "Epoch 36/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3554 - acc: 0.1065 - val_loss: 4.3215 - val_acc: 0.1053\n",
      "Epoch 37/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3551 - acc: 0.1060 - val_loss: 4.4305 - val_acc: 0.0925\n",
      "Epoch 38/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3612 - acc: 0.1062 - val_loss: 4.3812 - val_acc: 0.1010\n",
      "Epoch 39/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3585 - acc: 0.1065 - val_loss: 4.3681 - val_acc: 0.1030\n",
      "Epoch 40/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3649 - acc: 0.1059 - val_loss: 4.3717 - val_acc: 0.0966\n",
      "Epoch 41/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3663 - acc: 0.1059 - val_loss: 4.2144 - val_acc: 0.1163\n",
      "Epoch 42/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3683 - acc: 0.1050 - val_loss: 4.6620 - val_acc: 0.0771\n",
      "Epoch 43/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3685 - acc: 0.1065 - val_loss: 4.2637 - val_acc: 0.1100\n",
      "Epoch 44/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3692 - acc: 0.1041 - val_loss: 4.4826 - val_acc: 0.0937\n",
      "Epoch 45/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3709 - acc: 0.1054 - val_loss: 4.4205 - val_acc: 0.0958\n",
      "Epoch 46/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3698 - acc: 0.1046 - val_loss: 4.4221 - val_acc: 0.0907\n",
      "Epoch 47/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 4.3792 - acc: 0.1034 - val_loss: 4.3911 - val_acc: 0.0961\n",
      "Epoch 48/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3795 - acc: 0.1044 - val_loss: 4.4331 - val_acc: 0.0910\n",
      "Epoch 49/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3803 - acc: 0.1042 - val_loss: 4.3320 - val_acc: 0.1034\n",
      "Epoch 50/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3815 - acc: 0.1041 - val_loss: 4.4073 - val_acc: 0.0900\n",
      "10000/10000 [==============================] - 1s 129us/step\n",
      "Test loss: 4.407295335388183\n",
      "Test accuracy: 0.09\n",
      "Runtime: 1262.6598680019379\n",
      "Training Adam optimizer\n",
      "Epoch 1/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.0965 - acc: 0.0178 - val_loss: 4.8952 - val_acc: 0.0299\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8205 - acc: 0.0417 - val_loss: 4.7447 - val_acc: 0.0509\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6866 - acc: 0.0571 - val_loss: 4.5761 - val_acc: 0.0669\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6143 - acc: 0.0668 - val_loss: 4.5785 - val_acc: 0.0728\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 4.5593 - acc: 0.0725 - val_loss: 4.4372 - val_acc: 0.0892\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5140 - acc: 0.0789 - val_loss: 4.4026 - val_acc: 0.0900\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4832 - acc: 0.0830 - val_loss: 4.3598 - val_acc: 0.0928\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4575 - acc: 0.0864 - val_loss: 4.3902 - val_acc: 0.0927\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4373 - acc: 0.0878 - val_loss: 4.3357 - val_acc: 0.0946\n",
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 4.4237 - acc: 0.0921 - val_loss: 4.2881 - val_acc: 0.1063\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4028 - acc: 0.0947 - val_loss: 4.2807 - val_acc: 0.1066\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3895 - acc: 0.0939 - val_loss: 4.2722 - val_acc: 0.1095\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3794 - acc: 0.0961 - val_loss: 4.3096 - val_acc: 0.0978\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3692 - acc: 0.0978 - val_loss: 4.2746 - val_acc: 0.1081\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3567 - acc: 0.0993 - val_loss: 4.2947 - val_acc: 0.1042\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3505 - acc: 0.0995 - val_loss: 4.2971 - val_acc: 0.1047\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3405 - acc: 0.1010 - val_loss: 4.3333 - val_acc: 0.1008\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3293 - acc: 0.1024 - val_loss: 4.3080 - val_acc: 0.1045\n",
      "Epoch 19/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3224 - acc: 0.1049 - val_loss: 4.3303 - val_acc: 0.1013\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3223 - acc: 0.1042 - val_loss: 4.2657 - val_acc: 0.1072\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3099 - acc: 0.1043 - val_loss: 4.3233 - val_acc: 0.1013\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3095 - acc: 0.1053 - val_loss: 4.2924 - val_acc: 0.1033\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3026 - acc: 0.1067 - val_loss: 4.3895 - val_acc: 0.0916\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2946 - acc: 0.1070 - val_loss: 4.2164 - val_acc: 0.1128\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2899 - acc: 0.1073 - val_loss: 4.3408 - val_acc: 0.1001\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2903 - acc: 0.1092 - val_loss: 4.3740 - val_acc: 0.0985\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2843 - acc: 0.1087 - val_loss: 4.2863 - val_acc: 0.1046\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2851 - acc: 0.1079 - val_loss: 4.4584 - val_acc: 0.0920\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2809 - acc: 0.1087 - val_loss: 4.2481 - val_acc: 0.1099\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 4.2800 - acc: 0.1095 - val_loss: 4.3652 - val_acc: 0.1003\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2733 - acc: 0.1094 - val_loss: 4.4724 - val_acc: 0.0872\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2753 - acc: 0.1090 - val_loss: 4.4147 - val_acc: 0.0933\n",
      "Epoch 33/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2731 - acc: 0.1104 - val_loss: 4.4046 - val_acc: 0.0945\n",
      "Epoch 34/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2683 - acc: 0.1107 - val_loss: 4.2749 - val_acc: 0.1085\n",
      "Epoch 35/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2671 - acc: 0.1116 - val_loss: 4.4116 - val_acc: 0.0955\n",
      "Epoch 36/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2685 - acc: 0.1107 - val_loss: 4.3535 - val_acc: 0.1012\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2731 - acc: 0.1097 - val_loss: 4.3932 - val_acc: 0.0956\n",
      "Epoch 38/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2579 - acc: 0.1112 - val_loss: 4.3234 - val_acc: 0.1040\n",
      "Epoch 39/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2640 - acc: 0.1125 - val_loss: 4.4533 - val_acc: 0.0902\n",
      "Epoch 40/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2662 - acc: 0.1137 - val_loss: 4.1907 - val_acc: 0.1185\n",
      "Epoch 41/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2662 - acc: 0.1123 - val_loss: 4.2816 - val_acc: 0.1116\n",
      "Epoch 42/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2613 - acc: 0.1114 - val_loss: 4.3101 - val_acc: 0.1063\n",
      "Epoch 43/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2611 - acc: 0.1119 - val_loss: 4.2204 - val_acc: 0.1166\n",
      "Epoch 44/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2605 - acc: 0.1109 - val_loss: 4.2764 - val_acc: 0.1058\n",
      "Epoch 45/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2574 - acc: 0.1130 - val_loss: 4.2668 - val_acc: 0.1137\n",
      "Epoch 46/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2592 - acc: 0.1112 - val_loss: 4.3270 - val_acc: 0.1042\n",
      "Epoch 47/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2568 - acc: 0.1130 - val_loss: 4.3752 - val_acc: 0.1067\n",
      "Epoch 48/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2522 - acc: 0.1139 - val_loss: 4.4173 - val_acc: 0.1002\n",
      "Epoch 49/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2634 - acc: 0.1121 - val_loss: 4.3263 - val_acc: 0.1034\n",
      "Epoch 50/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2543 - acc: 0.1131 - val_loss: 4.3930 - val_acc: 0.1026\n",
      "10000/10000 [==============================] - 1s 134us/step\n",
      "Test loss: 4.3929524444580075\n",
      "Test accuracy: 0.1026\n",
      "Runtime: 1263.0200555324554\n",
      "Training Adamax optimizer\n",
      "Epoch 1/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.0345 - acc: 0.0236 - val_loss: 4.7009 - val_acc: 0.0564\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7080 - acc: 0.0551 - val_loss: 4.5171 - val_acc: 0.0746\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5419 - acc: 0.0752 - val_loss: 4.3767 - val_acc: 0.0951\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4225 - acc: 0.0909 - val_loss: 4.2819 - val_acc: 0.1037\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3299 - acc: 0.1016 - val_loss: 4.2269 - val_acc: 0.1137\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 4.2508 - acc: 0.1115 - val_loss: 4.2133 - val_acc: 0.1182\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1919 - acc: 0.1198 - val_loss: 4.2008 - val_acc: 0.1196\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1395 - acc: 0.1274 - val_loss: 4.2256 - val_acc: 0.1211\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0945 - acc: 0.1344 - val_loss: 3.9639 - val_acc: 0.1474\n",
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0608 - acc: 0.1384 - val_loss: 4.0020 - val_acc: 0.1452\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0259 - acc: 0.1429 - val_loss: 4.0745 - val_acc: 0.1375\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0005 - acc: 0.1482 - val_loss: 4.0282 - val_acc: 0.1439\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9784 - acc: 0.1506 - val_loss: 4.1686 - val_acc: 0.1372\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9515 - acc: 0.1537 - val_loss: 4.0776 - val_acc: 0.1390\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9394 - acc: 0.1557 - val_loss: 3.9187 - val_acc: 0.1604\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9201 - acc: 0.1574 - val_loss: 4.1063 - val_acc: 0.1400\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9023 - acc: 0.1612 - val_loss: 4.0555 - val_acc: 0.1416\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8921 - acc: 0.1617 - val_loss: 3.9654 - val_acc: 0.1545\n",
      "Epoch 19/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8796 - acc: 0.1643 - val_loss: 4.1267 - val_acc: 0.1429\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8709 - acc: 0.1657 - val_loss: 4.0197 - val_acc: 0.1494\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 3.8592 - acc: 0.1653 - val_loss: 3.9879 - val_acc: 0.1573\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8468 - acc: 0.1691 - val_loss: 4.0342 - val_acc: 0.1536\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 3.8388 - acc: 0.1703 - val_loss: 3.9356 - val_acc: 0.1614\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8321 - acc: 0.1708 - val_loss: 4.1513 - val_acc: 0.1426\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8268 - acc: 0.1724 - val_loss: 4.1608 - val_acc: 0.1414\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8159 - acc: 0.1736 - val_loss: 3.9125 - val_acc: 0.1675\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8090 - acc: 0.1752 - val_loss: 3.9005 - val_acc: 0.1616\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8079 - acc: 0.1755 - val_loss: 3.9510 - val_acc: 0.1600\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8002 - acc: 0.1782 - val_loss: 3.9206 - val_acc: 0.1619\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7930 - acc: 0.1788 - val_loss: 3.9662 - val_acc: 0.1561\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7848 - acc: 0.1792 - val_loss: 3.9301 - val_acc: 0.1612\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7789 - acc: 0.1785 - val_loss: 4.0269 - val_acc: 0.1534\n",
      "Epoch 33/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7763 - acc: 0.1813 - val_loss: 3.8668 - val_acc: 0.1657\n",
      "Epoch 34/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7723 - acc: 0.1813 - val_loss: 3.9717 - val_acc: 0.1627\n",
      "Epoch 35/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7678 - acc: 0.1807 - val_loss: 3.9299 - val_acc: 0.1635\n",
      "Epoch 36/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7651 - acc: 0.1815 - val_loss: 4.0389 - val_acc: 0.1567\n",
      "Epoch 37/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7609 - acc: 0.1834 - val_loss: 4.0134 - val_acc: 0.1567\n",
      "Epoch 38/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7574 - acc: 0.1839 - val_loss: 3.9903 - val_acc: 0.1637\n",
      "Epoch 39/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7574 - acc: 0.1833 - val_loss: 3.9944 - val_acc: 0.1603\n",
      "Epoch 40/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 3.7556 - acc: 0.1844 - val_loss: 3.9192 - val_acc: 0.1669\n",
      "Epoch 41/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7541 - acc: 0.1847 - val_loss: 4.1489 - val_acc: 0.1478\n",
      "Epoch 42/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7437 - acc: 0.1858 - val_loss: 4.0003 - val_acc: 0.1605\n",
      "Epoch 43/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7446 - acc: 0.1870 - val_loss: 3.9191 - val_acc: 0.1681\n",
      "Epoch 44/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7437 - acc: 0.1851 - val_loss: 4.1093 - val_acc: 0.1461\n",
      "Epoch 45/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7355 - acc: 0.1878 - val_loss: 3.8030 - val_acc: 0.1830\n",
      "Epoch 46/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7348 - acc: 0.1875 - val_loss: 4.1099 - val_acc: 0.1473\n",
      "Epoch 47/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7377 - acc: 0.1880 - val_loss: 3.9745 - val_acc: 0.1631\n",
      "Epoch 48/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7319 - acc: 0.1881 - val_loss: 3.8988 - val_acc: 0.1699\n",
      "Epoch 49/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7341 - acc: 0.1887 - val_loss: 3.8352 - val_acc: 0.1756\n",
      "Epoch 50/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7227 - acc: 0.1888 - val_loss: 3.8749 - val_acc: 0.1749\n",
      "10000/10000 [==============================] - 1s 137us/step\n",
      "Test loss: 3.8749251670837404\n",
      "Test accuracy: 0.1749\n",
      "Runtime: 1265.916042804718\n",
      "Training Nadam optimizer\n",
      "Epoch 1/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3004 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 2/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0046 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 3/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0045 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 4/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 5/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 6/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 7/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3001 - acc: 0.0045 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 8/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0045 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 9/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3009 - acc: 0.0046 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 10/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0043 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 11/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0046 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 12/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3001 - acc: 0.0044 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 13/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3001 - acc: 0.0044 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 14/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 15/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 16/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0048 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 17/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 18/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3002 - acc: 0.0045 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 19/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 20/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0046 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 21/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0046 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 22/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3001 - acc: 0.0044 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 23/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0043 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 24/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2986 - val_acc: 0.0050\n",
      "Epoch 25/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 26/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3001 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 27/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0045 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 28/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0045 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 29/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0045 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 30/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3003 - acc: 0.0043 - val_loss: 5.2986 - val_acc: 0.0050\n",
      "Epoch 31/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0045 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 32/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 33/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3003 - acc: 0.0043 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 34/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 35/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0046 - val_loss: 5.2986 - val_acc: 0.0050\n",
      "Epoch 36/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 37/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 38/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0046 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 39/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0042 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 40/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 41/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3002 - acc: 0.0044 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 42/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2986 - val_acc: 0.0050\n",
      "Epoch 43/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3001 - acc: 0.0046 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 44/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0043 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 45/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0047 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 46/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0048 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 47/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3002 - acc: 0.0042 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 48/50\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.3002 - acc: 0.0046 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "Epoch 49/50\n",
      "3125/3125 [==============================] - 27s 9ms/step - loss: 5.3001 - acc: 0.0049 - val_loss: 5.2988 - val_acc: 0.0050\n",
      "Epoch 50/50\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.3002 - acc: 0.0043 - val_loss: 5.2987 - val_acc: 0.0050\n",
      "10000/10000 [==============================] - 1s 132us/step\n",
      "Test loss: 5.298713544464111\n",
      "Test accuracy: 0.005\n",
      "Runtime: 1269.412302017212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opts = [('SGD', SGD()), ('RMSprop', RMSprop()), ('Adagrad', Adagrad()), ('Adadelta', Adadelta()), \n",
    "        ('Adam', Adam()), ('Adamax', Adamax()), ('Nadam', Nadam())]\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 200\n",
    "epochs = 50\n",
    "\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "\n",
    "for name, opt in opts:\n",
    "    print('Training ' + name + ' optimizer')\n",
    "    logs = \"logs/optimizer/\"+name\n",
    "    tensorboard = TensorBoard(log_dir=logs)\n",
    "    \n",
    "    model_name = name + '_keras_imagenet200_base.h5'\n",
    "    \n",
    "    model_base = Sequential()\n",
    "    model_base.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=train_images.shape[1:]))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Conv2D(32, (3, 3)))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_base.add(Dropout(0.25))\n",
    "\n",
    "    model_base.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Conv2D(64, (3, 3)))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_base.add(Dropout(0.25))\n",
    "\n",
    "    model_base.add(Flatten())\n",
    "    model_base.add(Dense(512))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(0.5))\n",
    "    model_base.add(Dense(num_classes))\n",
    "    model_base.add(Activation('softmax'))\n",
    "    \n",
    "    # Let's train the model using RMSprop\n",
    "    model_base.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_images)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model_base.fit_generator(datagen.flow(train_images, y_train, batch_size=batch_size),\n",
    "                                     epochs=epochs,\n",
    "                                     validation_data=(val_images, y_test),\n",
    "                                     workers=4,\n",
    "                                     steps_per_epoch=len(train_images)/batch_size, \n",
    "                                     callbacks=[tensorboard])\n",
    "    \n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model_base.save(model_path)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model_base.evaluate(val_images, y_test, verbose=1)\n",
    "\n",
    "    end = time()\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    print('Runtime:', str(end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGD optimizer\n",
      "Epoch 1/100\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 5.2978 - acc: 0.0048 - val_loss: 5.2914 - val_acc: 0.0053\n",
      "Epoch 2/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.2566 - acc: 0.0070 - val_loss: 5.2114 - val_acc: 0.0113\n",
      "Epoch 3/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.1814 - acc: 0.0107 - val_loss: 5.1011 - val_acc: 0.0162\n",
      "Epoch 4/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.1120 - acc: 0.0160 - val_loss: 5.0289 - val_acc: 0.0247\n",
      "Epoch 5/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.0430 - acc: 0.0225 - val_loss: 4.9140 - val_acc: 0.0413\n",
      "Epoch 6/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.9505 - acc: 0.0305 - val_loss: 4.7935 - val_acc: 0.0493\n",
      "Epoch 7/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.8572 - acc: 0.0385 - val_loss: 4.6980 - val_acc: 0.0610\n",
      "Epoch 8/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7894 - acc: 0.0457 - val_loss: 4.6174 - val_acc: 0.0700\n",
      "Epoch 9/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7283 - acc: 0.0530 - val_loss: 4.5622 - val_acc: 0.0761\n",
      "Epoch 10/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6762 - acc: 0.0597 - val_loss: 4.5028 - val_acc: 0.0826\n",
      "Epoch 11/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.6283 - acc: 0.0658 - val_loss: 4.4490 - val_acc: 0.0928\n",
      "Epoch 12/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5856 - acc: 0.0707 - val_loss: 4.4542 - val_acc: 0.0903\n",
      "Epoch 13/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5448 - acc: 0.0769 - val_loss: 4.3673 - val_acc: 0.1004\n",
      "Epoch 14/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5090 - acc: 0.0809 - val_loss: 4.3171 - val_acc: 0.1106\n",
      "Epoch 15/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4733 - acc: 0.0854 - val_loss: 4.3042 - val_acc: 0.1090\n",
      "Epoch 16/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4412 - acc: 0.0906 - val_loss: 4.2935 - val_acc: 0.1074\n",
      "Epoch 17/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4113 - acc: 0.0938 - val_loss: 4.2629 - val_acc: 0.1123\n",
      "Epoch 18/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3824 - acc: 0.0975 - val_loss: 4.2421 - val_acc: 0.1155\n",
      "Epoch 19/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3581 - acc: 0.1006 - val_loss: 4.1927 - val_acc: 0.1182\n",
      "Epoch 20/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3344 - acc: 0.1043 - val_loss: 4.1509 - val_acc: 0.1295\n",
      "Epoch 21/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.3082 - acc: 0.1055 - val_loss: 4.1397 - val_acc: 0.1275\n",
      "Epoch 22/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2863 - acc: 0.1096 - val_loss: 4.1648 - val_acc: 0.1215\n",
      "Epoch 23/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2663 - acc: 0.1119 - val_loss: 4.2415 - val_acc: 0.1146\n",
      "Epoch 24/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2432 - acc: 0.1153 - val_loss: 4.1483 - val_acc: 0.1254\n",
      "Epoch 25/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2267 - acc: 0.1169 - val_loss: 4.0898 - val_acc: 0.1337\n",
      "Epoch 26/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2084 - acc: 0.1197 - val_loss: 4.1234 - val_acc: 0.1314\n",
      "Epoch 27/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1926 - acc: 0.1211 - val_loss: 4.1774 - val_acc: 0.1235\n",
      "Epoch 28/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1739 - acc: 0.1240 - val_loss: 4.0991 - val_acc: 0.1350\n",
      "Epoch 29/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1597 - acc: 0.1264 - val_loss: 4.1126 - val_acc: 0.1360\n",
      "Epoch 30/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1396 - acc: 0.1281 - val_loss: 4.1366 - val_acc: 0.1277\n",
      "Epoch 31/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1227 - acc: 0.1307 - val_loss: 4.0530 - val_acc: 0.1402\n",
      "Epoch 32/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1140 - acc: 0.1315 - val_loss: 4.1413 - val_acc: 0.1268\n",
      "Epoch 33/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1002 - acc: 0.1344 - val_loss: 4.0272 - val_acc: 0.1422\n",
      "Epoch 34/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0838 - acc: 0.1379 - val_loss: 4.0143 - val_acc: 0.1459\n",
      "Epoch 35/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0721 - acc: 0.1371 - val_loss: 4.1575 - val_acc: 0.1293\n",
      "Epoch 36/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0598 - acc: 0.1398 - val_loss: 4.0967 - val_acc: 0.1327\n",
      "Epoch 37/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0470 - acc: 0.1404 - val_loss: 4.0948 - val_acc: 0.1373\n",
      "Epoch 38/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0394 - acc: 0.1425 - val_loss: 4.0772 - val_acc: 0.1373\n",
      "Epoch 39/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0176 - acc: 0.1471 - val_loss: 3.9937 - val_acc: 0.1519\n",
      "Epoch 40/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0118 - acc: 0.1464 - val_loss: 4.1441 - val_acc: 0.1336\n",
      "Epoch 41/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0005 - acc: 0.1483 - val_loss: 4.0351 - val_acc: 0.1436\n",
      "Epoch 42/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9909 - acc: 0.1485 - val_loss: 3.9878 - val_acc: 0.1510\n",
      "Epoch 43/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9766 - acc: 0.1505 - val_loss: 3.9808 - val_acc: 0.1493\n",
      "Epoch 44/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9693 - acc: 0.1498 - val_loss: 4.0263 - val_acc: 0.1437\n",
      "Epoch 45/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9614 - acc: 0.1523 - val_loss: 4.1672 - val_acc: 0.1319\n",
      "Epoch 46/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9497 - acc: 0.1552 - val_loss: 4.1010 - val_acc: 0.1381\n",
      "Epoch 47/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9425 - acc: 0.1565 - val_loss: 4.0455 - val_acc: 0.1410\n",
      "Epoch 48/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9339 - acc: 0.1567 - val_loss: 4.0941 - val_acc: 0.1360\n",
      "Epoch 49/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9260 - acc: 0.1568 - val_loss: 3.9734 - val_acc: 0.1506\n",
      "Epoch 50/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9162 - acc: 0.1598 - val_loss: 3.9749 - val_acc: 0.1555\n",
      "Epoch 51/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9083 - acc: 0.1596 - val_loss: 4.0289 - val_acc: 0.1436\n",
      "Epoch 52/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8988 - acc: 0.1603 - val_loss: 3.9225 - val_acc: 0.1585\n",
      "Epoch 53/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8943 - acc: 0.1613 - val_loss: 4.0598 - val_acc: 0.1417\n",
      "Epoch 54/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8823 - acc: 0.1633 - val_loss: 4.1442 - val_acc: 0.1352\n",
      "Epoch 55/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8770 - acc: 0.1638 - val_loss: 3.9457 - val_acc: 0.1568\n",
      "Epoch 56/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8722 - acc: 0.1652 - val_loss: 4.0240 - val_acc: 0.1449\n",
      "Epoch 57/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8628 - acc: 0.1660 - val_loss: 3.9512 - val_acc: 0.1569\n",
      "Epoch 58/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8568 - acc: 0.1661 - val_loss: 3.9511 - val_acc: 0.1550\n",
      "Epoch 59/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8485 - acc: 0.1685 - val_loss: 3.8387 - val_acc: 0.1743\n",
      "Epoch 60/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8400 - acc: 0.1714 - val_loss: 3.8834 - val_acc: 0.1654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8347 - acc: 0.1702 - val_loss: 3.9722 - val_acc: 0.1563\n",
      "Epoch 62/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8319 - acc: 0.1726 - val_loss: 3.8830 - val_acc: 0.1672\n",
      "Epoch 63/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8224 - acc: 0.1729 - val_loss: 3.9292 - val_acc: 0.1556\n",
      "Epoch 64/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8196 - acc: 0.1738 - val_loss: 3.9330 - val_acc: 0.1590\n",
      "Epoch 65/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8114 - acc: 0.1730 - val_loss: 3.9807 - val_acc: 0.1548\n",
      "Epoch 66/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8100 - acc: 0.1739 - val_loss: 3.9770 - val_acc: 0.1522\n",
      "Epoch 67/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7995 - acc: 0.1750 - val_loss: 3.9320 - val_acc: 0.1594\n",
      "Epoch 68/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7882 - acc: 0.1770 - val_loss: 3.8741 - val_acc: 0.1684\n",
      "Epoch 69/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7821 - acc: 0.1797 - val_loss: 3.9608 - val_acc: 0.1546\n",
      "Epoch 70/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7825 - acc: 0.1788 - val_loss: 4.1162 - val_acc: 0.1451\n",
      "Epoch 71/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7777 - acc: 0.1796 - val_loss: 3.9798 - val_acc: 0.1558\n",
      "Epoch 72/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7682 - acc: 0.1804 - val_loss: 3.9615 - val_acc: 0.1563\n",
      "Epoch 73/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7646 - acc: 0.1801 - val_loss: 3.9591 - val_acc: 0.1600\n",
      "Epoch 74/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7652 - acc: 0.1812 - val_loss: 4.0223 - val_acc: 0.1529\n",
      "Epoch 75/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7545 - acc: 0.1809 - val_loss: 3.8710 - val_acc: 0.1691\n",
      "Epoch 76/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7545 - acc: 0.1826 - val_loss: 3.8765 - val_acc: 0.1691\n",
      "Epoch 77/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7423 - acc: 0.1846 - val_loss: 3.8410 - val_acc: 0.1736\n",
      "Epoch 78/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7458 - acc: 0.1840 - val_loss: 3.9373 - val_acc: 0.1610\n",
      "Epoch 79/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7404 - acc: 0.1837 - val_loss: 3.8815 - val_acc: 0.1686\n",
      "Epoch 80/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7358 - acc: 0.1840 - val_loss: 3.9605 - val_acc: 0.1574\n",
      "Epoch 81/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7234 - acc: 0.1864 - val_loss: 3.8920 - val_acc: 0.1668\n",
      "Epoch 82/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7255 - acc: 0.1871 - val_loss: 3.8221 - val_acc: 0.1797\n",
      "Epoch 83/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7218 - acc: 0.1880 - val_loss: 3.8588 - val_acc: 0.1742\n",
      "Epoch 84/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7143 - acc: 0.1881 - val_loss: 3.9173 - val_acc: 0.1651\n",
      "Epoch 85/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7152 - acc: 0.1908 - val_loss: 3.8380 - val_acc: 0.1724\n",
      "Epoch 86/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7110 - acc: 0.1900 - val_loss: 3.9085 - val_acc: 0.1645\n",
      "Epoch 87/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7016 - acc: 0.1900 - val_loss: 3.8850 - val_acc: 0.1690\n",
      "Epoch 88/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7003 - acc: 0.1909 - val_loss: 3.9769 - val_acc: 0.1528\n",
      "Epoch 89/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7003 - acc: 0.1912 - val_loss: 3.9775 - val_acc: 0.1574\n",
      "Epoch 90/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6968 - acc: 0.1912 - val_loss: 3.7551 - val_acc: 0.1833\n",
      "Epoch 91/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6935 - acc: 0.1923 - val_loss: 3.9341 - val_acc: 0.1621\n",
      "Epoch 92/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6860 - acc: 0.1916 - val_loss: 3.8939 - val_acc: 0.1653\n",
      "Epoch 93/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6816 - acc: 0.1947 - val_loss: 3.8924 - val_acc: 0.1737\n",
      "Epoch 94/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6794 - acc: 0.1922 - val_loss: 3.8845 - val_acc: 0.1668\n",
      "Epoch 95/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6752 - acc: 0.1939 - val_loss: 3.8993 - val_acc: 0.1630\n",
      "Epoch 96/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6728 - acc: 0.1944 - val_loss: 3.8600 - val_acc: 0.1754\n",
      "Epoch 97/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6650 - acc: 0.1948 - val_loss: 3.9931 - val_acc: 0.1585\n",
      "Epoch 98/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6646 - acc: 0.1953 - val_loss: 4.0023 - val_acc: 0.1559\n",
      "Epoch 99/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6631 - acc: 0.1972 - val_loss: 3.9016 - val_acc: 0.1655\n",
      "Epoch 100/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6604 - acc: 0.1969 - val_loss: 3.7905 - val_acc: 0.1810\n",
      "10000/10000 [==============================] - 1s 125us/step\n",
      "Test loss: 3.790474868011475\n",
      "Test accuracy: 0.181\n",
      "Runtime: 2496.592635154724\n",
      "Training Adamax optimizer\n",
      "Epoch 1/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 5.1291 - acc: 0.0164 - val_loss: 4.8706 - val_acc: 0.0384\n",
      "Epoch 2/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.7691 - acc: 0.0489 - val_loss: 4.5008 - val_acc: 0.0828\n",
      "Epoch 3/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.5444 - acc: 0.0749 - val_loss: 4.3769 - val_acc: 0.0910\n",
      "Epoch 4/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.4032 - acc: 0.0927 - val_loss: 4.2911 - val_acc: 0.1041\n",
      "Epoch 5/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2964 - acc: 0.1059 - val_loss: 4.1664 - val_acc: 0.1213\n",
      "Epoch 6/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.2246 - acc: 0.1167 - val_loss: 4.1426 - val_acc: 0.1256\n",
      "Epoch 7/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1653 - acc: 0.1238 - val_loss: 4.1263 - val_acc: 0.1234\n",
      "Epoch 8/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.1192 - acc: 0.1286 - val_loss: 4.0455 - val_acc: 0.1377\n",
      "Epoch 9/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0751 - acc: 0.1359 - val_loss: 4.0399 - val_acc: 0.1353\n",
      "Epoch 10/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 4.0450 - acc: 0.1399 - val_loss: 4.0614 - val_acc: 0.1362\n",
      "Epoch 11/100\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 4.0207 - acc: 0.1425 - val_loss: 4.1567 - val_acc: 0.1283\n",
      "Epoch 12/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9927 - acc: 0.1483 - val_loss: 4.1383 - val_acc: 0.1296\n",
      "Epoch 13/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9742 - acc: 0.1506 - val_loss: 4.0987 - val_acc: 0.1343\n",
      "Epoch 14/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9543 - acc: 0.1522 - val_loss: 3.9434 - val_acc: 0.1588\n",
      "Epoch 15/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9341 - acc: 0.1563 - val_loss: 4.0152 - val_acc: 0.1451\n",
      "Epoch 16/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.9177 - acc: 0.1590 - val_loss: 4.0619 - val_acc: 0.1411\n",
      "Epoch 17/100\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 3.9010 - acc: 0.1610 - val_loss: 4.0138 - val_acc: 0.1446\n",
      "Epoch 18/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8924 - acc: 0.1619 - val_loss: 3.9476 - val_acc: 0.1545\n",
      "Epoch 19/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8811 - acc: 0.1638 - val_loss: 3.9382 - val_acc: 0.1564\n",
      "Epoch 20/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8689 - acc: 0.1661 - val_loss: 3.9279 - val_acc: 0.1599\n",
      "Epoch 21/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8625 - acc: 0.1668 - val_loss: 4.0003 - val_acc: 0.1500\n",
      "Epoch 22/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8516 - acc: 0.1675 - val_loss: 4.0299 - val_acc: 0.1457\n",
      "Epoch 23/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8395 - acc: 0.1702 - val_loss: 3.8666 - val_acc: 0.1681\n",
      "Epoch 24/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8369 - acc: 0.1707 - val_loss: 3.9074 - val_acc: 0.1637\n",
      "Epoch 25/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8252 - acc: 0.1715 - val_loss: 4.0933 - val_acc: 0.1457\n",
      "Epoch 26/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8224 - acc: 0.1725 - val_loss: 4.0738 - val_acc: 0.1420\n",
      "Epoch 27/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8154 - acc: 0.1747 - val_loss: 3.8028 - val_acc: 0.1772\n",
      "Epoch 28/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8039 - acc: 0.1757 - val_loss: 4.0321 - val_acc: 0.1513\n",
      "Epoch 29/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.8054 - acc: 0.1762 - val_loss: 4.0096 - val_acc: 0.1510\n",
      "Epoch 30/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7947 - acc: 0.1771 - val_loss: 3.9246 - val_acc: 0.1626\n",
      "Epoch 31/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7942 - acc: 0.1786 - val_loss: 4.0129 - val_acc: 0.1511\n",
      "Epoch 32/100\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 3.7924 - acc: 0.1782 - val_loss: 3.8847 - val_acc: 0.1673\n",
      "Epoch 33/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7831 - acc: 0.1791 - val_loss: 3.9709 - val_acc: 0.1626\n",
      "Epoch 34/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7784 - acc: 0.1799 - val_loss: 3.8866 - val_acc: 0.1677\n",
      "Epoch 35/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7759 - acc: 0.1823 - val_loss: 3.9920 - val_acc: 0.1593\n",
      "Epoch 36/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7766 - acc: 0.1806 - val_loss: 4.1213 - val_acc: 0.1431\n",
      "Epoch 37/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7722 - acc: 0.1798 - val_loss: 3.8062 - val_acc: 0.1727\n",
      "Epoch 38/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7652 - acc: 0.1831 - val_loss: 3.9379 - val_acc: 0.1572\n",
      "Epoch 39/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7606 - acc: 0.1821 - val_loss: 3.9975 - val_acc: 0.1596\n",
      "Epoch 40/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7560 - acc: 0.1825 - val_loss: 3.8917 - val_acc: 0.1708\n",
      "Epoch 41/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7522 - acc: 0.1839 - val_loss: 3.9538 - val_acc: 0.1620\n",
      "Epoch 42/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7522 - acc: 0.1845 - val_loss: 3.8968 - val_acc: 0.1706\n",
      "Epoch 43/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7480 - acc: 0.1858 - val_loss: 3.8617 - val_acc: 0.1715\n",
      "Epoch 44/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7479 - acc: 0.1845 - val_loss: 3.9688 - val_acc: 0.1595\n",
      "Epoch 45/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7479 - acc: 0.1845 - val_loss: 3.8251 - val_acc: 0.1776\n",
      "Epoch 46/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7434 - acc: 0.1862 - val_loss: 3.8918 - val_acc: 0.1723\n",
      "Epoch 47/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7402 - acc: 0.1872 - val_loss: 4.0433 - val_acc: 0.1557\n",
      "Epoch 48/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7381 - acc: 0.1862 - val_loss: 3.9057 - val_acc: 0.1766\n",
      "Epoch 49/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7375 - acc: 0.1862 - val_loss: 3.9328 - val_acc: 0.1658\n",
      "Epoch 50/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7376 - acc: 0.1870 - val_loss: 4.0422 - val_acc: 0.1523\n",
      "Epoch 51/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7350 - acc: 0.1875 - val_loss: 3.9205 - val_acc: 0.1641\n",
      "Epoch 52/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7339 - acc: 0.1878 - val_loss: 3.8939 - val_acc: 0.1672\n",
      "Epoch 53/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7281 - acc: 0.1895 - val_loss: 4.0192 - val_acc: 0.1596\n",
      "Epoch 54/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7233 - acc: 0.1893 - val_loss: 3.9198 - val_acc: 0.1693\n",
      "Epoch 55/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7256 - acc: 0.1874 - val_loss: 4.0742 - val_acc: 0.1531\n",
      "Epoch 56/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7251 - acc: 0.1902 - val_loss: 3.8804 - val_acc: 0.1728\n",
      "Epoch 57/100\n",
      "3125/3125 [==============================] - 26s 8ms/step - loss: 3.7222 - acc: 0.1905 - val_loss: 4.0270 - val_acc: 0.1568\n",
      "Epoch 58/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7159 - acc: 0.1898 - val_loss: 3.9490 - val_acc: 0.1649\n",
      "Epoch 59/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7179 - acc: 0.1901 - val_loss: 4.0360 - val_acc: 0.1524\n",
      "Epoch 60/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7153 - acc: 0.1907 - val_loss: 3.9637 - val_acc: 0.1684\n",
      "Epoch 61/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7176 - acc: 0.1900 - val_loss: 3.8738 - val_acc: 0.1798\n",
      "Epoch 62/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7161 - acc: 0.1902 - val_loss: 3.8691 - val_acc: 0.1742\n",
      "Epoch 63/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7204 - acc: 0.1900 - val_loss: 4.0667 - val_acc: 0.1526\n",
      "Epoch 64/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7159 - acc: 0.1914 - val_loss: 4.1144 - val_acc: 0.1524\n",
      "Epoch 65/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7105 - acc: 0.1925 - val_loss: 3.8725 - val_acc: 0.1738\n",
      "Epoch 66/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7128 - acc: 0.1907 - val_loss: 4.1705 - val_acc: 0.1569\n",
      "Epoch 67/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7189 - acc: 0.1898 - val_loss: 4.0329 - val_acc: 0.1597\n",
      "Epoch 68/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7118 - acc: 0.1907 - val_loss: 3.8534 - val_acc: 0.1775\n",
      "Epoch 69/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7091 - acc: 0.1916 - val_loss: 3.8631 - val_acc: 0.1779\n",
      "Epoch 70/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7055 - acc: 0.1946 - val_loss: 3.8624 - val_acc: 0.1856\n",
      "Epoch 71/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7107 - acc: 0.1921 - val_loss: 3.9683 - val_acc: 0.1618\n",
      "Epoch 72/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7104 - acc: 0.1933 - val_loss: 3.8502 - val_acc: 0.1864\n",
      "Epoch 73/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7042 - acc: 0.1936 - val_loss: 3.8229 - val_acc: 0.1819\n",
      "Epoch 74/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7108 - acc: 0.1924 - val_loss: 3.8543 - val_acc: 0.1724\n",
      "Epoch 75/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7112 - acc: 0.1929 - val_loss: 3.9549 - val_acc: 0.1662\n",
      "Epoch 76/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7030 - acc: 0.1928 - val_loss: 3.9490 - val_acc: 0.1732\n",
      "Epoch 77/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7114 - acc: 0.1927 - val_loss: 3.7943 - val_acc: 0.1828\n",
      "Epoch 78/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7042 - acc: 0.1932 - val_loss: 3.9577 - val_acc: 0.1636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7080 - acc: 0.1926 - val_loss: 3.9969 - val_acc: 0.1618\n",
      "Epoch 80/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7064 - acc: 0.1939 - val_loss: 3.9885 - val_acc: 0.1655\n",
      "Epoch 81/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7044 - acc: 0.1917 - val_loss: 3.9069 - val_acc: 0.1779\n",
      "Epoch 82/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7050 - acc: 0.1929 - val_loss: 3.8028 - val_acc: 0.1826\n",
      "Epoch 83/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6972 - acc: 0.1929 - val_loss: 3.9967 - val_acc: 0.1696\n",
      "Epoch 84/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7009 - acc: 0.1936 - val_loss: 3.8741 - val_acc: 0.1747\n",
      "Epoch 85/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7062 - acc: 0.1946 - val_loss: 3.8964 - val_acc: 0.1763\n",
      "Epoch 86/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7021 - acc: 0.1943 - val_loss: 4.0409 - val_acc: 0.1685\n",
      "Epoch 87/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6995 - acc: 0.1941 - val_loss: 3.8420 - val_acc: 0.1844\n",
      "Epoch 88/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6968 - acc: 0.1949 - val_loss: 3.9136 - val_acc: 0.1747\n",
      "Epoch 89/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7029 - acc: 0.1951 - val_loss: 3.9067 - val_acc: 0.1718\n",
      "Epoch 90/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6987 - acc: 0.1944 - val_loss: 3.8877 - val_acc: 0.1762\n",
      "Epoch 91/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6992 - acc: 0.1947 - val_loss: 4.0369 - val_acc: 0.1665\n",
      "Epoch 92/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7039 - acc: 0.1930 - val_loss: 4.0264 - val_acc: 0.1649\n",
      "Epoch 93/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7025 - acc: 0.1945 - val_loss: 3.9214 - val_acc: 0.1722\n",
      "Epoch 94/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7006 - acc: 0.1933 - val_loss: 4.0065 - val_acc: 0.1685\n",
      "Epoch 95/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7018 - acc: 0.1931 - val_loss: 3.8697 - val_acc: 0.1767\n",
      "Epoch 96/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6989 - acc: 0.1942 - val_loss: 4.0194 - val_acc: 0.1660\n",
      "Epoch 97/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6990 - acc: 0.1950 - val_loss: 3.9220 - val_acc: 0.1786\n",
      "Epoch 98/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6999 - acc: 0.1932 - val_loss: 4.0216 - val_acc: 0.1637\n",
      "Epoch 99/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.6951 - acc: 0.1958 - val_loss: 3.8394 - val_acc: 0.1862\n",
      "Epoch 100/100\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 3.7002 - acc: 0.1941 - val_loss: 3.9430 - val_acc: 0.1747\n",
      "10000/10000 [==============================] - 1s 125us/step\n",
      "Test loss: 3.9429873306274414\n",
      "Test accuracy: 0.1747\n",
      "Runtime: 2517.3177618980408\n"
     ]
    }
   ],
   "source": [
    "opts = [('SGD', SGD()), ('Adamax', Adamax())]\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 200\n",
    "epochs = 100\n",
    "\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "\n",
    "for name, opt in opts:\n",
    "    print('Training ' + name + ' optimizer')\n",
    "    logs = \"logs/optimizer/100/\"+name\n",
    "    tensorboard = TensorBoard(log_dir=logs)\n",
    "    \n",
    "    model_name = name + '_100_keras_imagenet200_base.h5'\n",
    "    \n",
    "    model_base = Sequential()\n",
    "    model_base.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=train_images.shape[1:]))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Conv2D(32, (3, 3)))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_base.add(Dropout(0.25))\n",
    "\n",
    "    model_base.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Conv2D(64, (3, 3)))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_base.add(Dropout(0.25))\n",
    "\n",
    "    model_base.add(Flatten())\n",
    "    model_base.add(Dense(512))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(0.5))\n",
    "    model_base.add(Dense(num_classes))\n",
    "    model_base.add(Activation('softmax'))\n",
    "    \n",
    "    # Let's train the model using RMSprop\n",
    "    model_base.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_images)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model_base.fit_generator(datagen.flow(train_images, y_train, batch_size=batch_size),\n",
    "                                     epochs=epochs,\n",
    "                                     validation_data=(val_images, y_test),\n",
    "                                     workers=4,\n",
    "                                     steps_per_epoch=len(train_images)/batch_size, \n",
    "                                     callbacks=[tensorboard])\n",
    "    \n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model_base.save(model_path)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model_base.evaluate(val_images, y_test, verbose=1)\n",
    "\n",
    "    end = time()\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    print('Runtime:', str(end-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: CIFAR 100 Starting Code\n",
    "https://andrewkruger.github.io/projects/2017-08-05-keras-convolutional-neural-network-for-cifar-100#the-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1562 [==============================] - 37s 23ms/step - loss: 4.8993 - acc: 0.0435 - val_loss: 4.4843 - val_acc: 0.0967\n",
      "Epoch 2/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 4.5560 - acc: 0.0845 - val_loss: 4.1693 - val_acc: 0.1394\n",
      "Epoch 3/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 4.4113 - acc: 0.1047 - val_loss: 4.0295 - val_acc: 0.1616\n",
      "Epoch 4/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 4.3023 - acc: 0.1209 - val_loss: 3.9087 - val_acc: 0.1765\n",
      "Epoch 5/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 4.2180 - acc: 0.1355 - val_loss: 3.8326 - val_acc: 0.1934\n",
      "Epoch 6/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 4.1376 - acc: 0.1467 - val_loss: 3.6963 - val_acc: 0.2128\n",
      "Epoch 7/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 4.0630 - acc: 0.1575 - val_loss: 3.7248 - val_acc: 0.2086\n",
      "Epoch 8/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.9884 - acc: 0.1697 - val_loss: 3.5665 - val_acc: 0.2326\n",
      "Epoch 9/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.9194 - acc: 0.1807 - val_loss: 3.5528 - val_acc: 0.2344\n",
      "Epoch 10/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.8554 - acc: 0.1916 - val_loss: 3.4688 - val_acc: 0.2528\n",
      "Epoch 11/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.7962 - acc: 0.2005 - val_loss: 3.4869 - val_acc: 0.2443\n",
      "Epoch 12/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.7338 - acc: 0.2112 - val_loss: 3.3742 - val_acc: 0.2629\n",
      "Epoch 13/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.6773 - acc: 0.2193 - val_loss: 3.3692 - val_acc: 0.2665\n",
      "Epoch 14/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.6210 - acc: 0.2305 - val_loss: 3.3832 - val_acc: 0.2642\n",
      "Epoch 15/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.5635 - acc: 0.2403 - val_loss: 3.2813 - val_acc: 0.2801\n",
      "Epoch 16/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.5113 - acc: 0.2495 - val_loss: 3.1987 - val_acc: 0.3019\n",
      "Epoch 17/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.4692 - acc: 0.2553 - val_loss: 3.1840 - val_acc: 0.2997\n",
      "Epoch 18/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.4238 - acc: 0.2641 - val_loss: 3.1785 - val_acc: 0.3054\n",
      "Epoch 19/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.3826 - acc: 0.2705 - val_loss: 3.0978 - val_acc: 0.3163\n",
      "Epoch 20/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.3336 - acc: 0.2794 - val_loss: 3.1142 - val_acc: 0.3165\n",
      "Epoch 21/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.3005 - acc: 0.2850 - val_loss: 3.0817 - val_acc: 0.3222\n",
      "Epoch 22/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.2708 - acc: 0.2896 - val_loss: 3.0994 - val_acc: 0.3189\n",
      "Epoch 23/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.2454 - acc: 0.2935 - val_loss: 3.0358 - val_acc: 0.3289\n",
      "Epoch 24/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.2162 - acc: 0.2994 - val_loss: 3.0077 - val_acc: 0.3302\n",
      "Epoch 25/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.1868 - acc: 0.3064 - val_loss: 3.0404 - val_acc: 0.3301\n",
      "Epoch 26/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.1576 - acc: 0.3118 - val_loss: 3.0314 - val_acc: 0.3328\n",
      "Epoch 27/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.1480 - acc: 0.3126 - val_loss: 3.0735 - val_acc: 0.3219\n",
      "Epoch 28/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.1358 - acc: 0.3179 - val_loss: 3.0167 - val_acc: 0.3358\n",
      "Epoch 29/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.1097 - acc: 0.3223 - val_loss: 3.0001 - val_acc: 0.3377\n",
      "Epoch 30/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.0882 - acc: 0.3269 - val_loss: 3.0353 - val_acc: 0.3308\n",
      "Epoch 31/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.0800 - acc: 0.3282 - val_loss: 2.9790 - val_acc: 0.3408\n",
      "Epoch 32/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.0598 - acc: 0.3320 - val_loss: 2.9907 - val_acc: 0.3415\n",
      "Epoch 33/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.0530 - acc: 0.3338 - val_loss: 3.0079 - val_acc: 0.3419\n",
      "Epoch 34/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.0387 - acc: 0.3373 - val_loss: 2.9635 - val_acc: 0.3500\n",
      "Epoch 35/200\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 3.0165 - acc: 0.3400 - val_loss: 3.0270 - val_acc: 0.3334\n",
      "Epoch 36/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.0012 - acc: 0.3450 - val_loss: 3.0350 - val_acc: 0.3380\n",
      "Epoch 37/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.9899 - acc: 0.3465 - val_loss: 2.9951 - val_acc: 0.3440\n",
      "Epoch 38/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.9722 - acc: 0.3484 - val_loss: 2.9374 - val_acc: 0.3548\n",
      "Epoch 39/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9608 - acc: 0.3525 - val_loss: 2.9105 - val_acc: 0.3605\n",
      "Epoch 40/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.9512 - acc: 0.3545 - val_loss: 2.9878 - val_acc: 0.3533\n",
      "Epoch 41/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9369 - acc: 0.3585 - val_loss: 2.9070 - val_acc: 0.3597\n",
      "Epoch 42/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.9199 - acc: 0.3622 - val_loss: 2.8952 - val_acc: 0.3668\n",
      "Epoch 43/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.8975 - acc: 0.3656 - val_loss: 3.0372 - val_acc: 0.3438\n",
      "Epoch 44/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8893 - acc: 0.3674 - val_loss: 3.0559 - val_acc: 0.3443\n",
      "Epoch 45/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.8759 - acc: 0.3692 - val_loss: 2.9639 - val_acc: 0.3567\n",
      "Epoch 46/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.8640 - acc: 0.3728 - val_loss: 3.0046 - val_acc: 0.3547\n",
      "Epoch 47/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.8545 - acc: 0.3763 - val_loss: 3.0043 - val_acc: 0.3493\n",
      "Epoch 48/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8438 - acc: 0.3763 - val_loss: 2.8923 - val_acc: 0.3715\n",
      "Epoch 49/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.8304 - acc: 0.3790 - val_loss: 2.9437 - val_acc: 0.3625\n",
      "Epoch 50/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8129 - acc: 0.3837 - val_loss: 2.9149 - val_acc: 0.3658\n",
      "Epoch 51/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.8059 - acc: 0.3848 - val_loss: 2.9522 - val_acc: 0.3644\n",
      "Epoch 52/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.7954 - acc: 0.3898 - val_loss: 3.0216 - val_acc: 0.3515\n",
      "Epoch 53/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7880 - acc: 0.3877 - val_loss: 2.9950 - val_acc: 0.3572\n",
      "Epoch 54/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7791 - acc: 0.3898 - val_loss: 2.9978 - val_acc: 0.3580\n",
      "Epoch 55/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.7738 - acc: 0.3917 - val_loss: 2.9709 - val_acc: 0.3578\n",
      "Epoch 56/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7673 - acc: 0.3929 - val_loss: 3.0368 - val_acc: 0.3540\n",
      "Epoch 57/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7659 - acc: 0.3951 - val_loss: 2.9760 - val_acc: 0.3641\n",
      "Epoch 58/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.7573 - acc: 0.3976 - val_loss: 2.9776 - val_acc: 0.3576\n",
      "Epoch 59/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7560 - acc: 0.3981 - val_loss: 3.0736 - val_acc: 0.3450\n",
      "Epoch 60/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7576 - acc: 0.3988 - val_loss: 2.9390 - val_acc: 0.3716\n",
      "Epoch 61/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.7528 - acc: 0.3991 - val_loss: 3.0033 - val_acc: 0.3593\n",
      "Epoch 62/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.7647 - acc: 0.3992 - val_loss: 2.9664 - val_acc: 0.3631\n",
      "Epoch 63/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.7696 - acc: 0.3977 - val_loss: 3.0567 - val_acc: 0.3555\n",
      "Epoch 64/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.7736 - acc: 0.3988 - val_loss: 3.0366 - val_acc: 0.3516\n",
      "Epoch 65/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7925 - acc: 0.3971 - val_loss: 3.0871 - val_acc: 0.3480\n",
      "Epoch 66/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.7961 - acc: 0.3955 - val_loss: 3.0598 - val_acc: 0.3532\n",
      "Epoch 67/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.8138 - acc: 0.3935 - val_loss: 2.9920 - val_acc: 0.3600\n",
      "Epoch 68/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8248 - acc: 0.3935 - val_loss: 2.9283 - val_acc: 0.3707\n",
      "Epoch 69/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8473 - acc: 0.3882 - val_loss: 3.1230 - val_acc: 0.3392\n",
      "Epoch 70/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8550 - acc: 0.3877 - val_loss: 2.9637 - val_acc: 0.3656\n",
      "Epoch 71/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8858 - acc: 0.3838 - val_loss: 2.9983 - val_acc: 0.3620\n",
      "Epoch 72/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8936 - acc: 0.3817 - val_loss: 2.9701 - val_acc: 0.3629\n",
      "Epoch 73/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.9322 - acc: 0.3770 - val_loss: 3.0293 - val_acc: 0.3511\n",
      "Epoch 74/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 2.9487 - acc: 0.3728 - val_loss: 3.0059 - val_acc: 0.3583\n",
      "Epoch 75/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9736 - acc: 0.3693 - val_loss: 3.0276 - val_acc: 0.3505\n",
      "Epoch 76/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0096 - acc: 0.3627 - val_loss: 3.0515 - val_acc: 0.3473\n",
      "Epoch 77/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.0316 - acc: 0.3566 - val_loss: 3.0713 - val_acc: 0.3386\n",
      "Epoch 78/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0733 - acc: 0.3512 - val_loss: 3.0653 - val_acc: 0.3499\n",
      "Epoch 79/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.0900 - acc: 0.3495 - val_loss: 3.0770 - val_acc: 0.3393\n",
      "Epoch 80/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.1401 - acc: 0.3408 - val_loss: 3.1108 - val_acc: 0.3363\n",
      "Epoch 81/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.1761 - acc: 0.3316 - val_loss: 3.1482 - val_acc: 0.3259\n",
      "Epoch 82/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2076 - acc: 0.3277 - val_loss: 3.2045 - val_acc: 0.3277\n",
      "Epoch 83/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2358 - acc: 0.3228 - val_loss: 3.1899 - val_acc: 0.3151\n",
      "Epoch 84/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2544 - acc: 0.3191 - val_loss: 3.2017 - val_acc: 0.3103\n",
      "Epoch 85/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2811 - acc: 0.3172 - val_loss: 3.2607 - val_acc: 0.3098\n",
      "Epoch 86/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2827 - acc: 0.3145 - val_loss: 3.2352 - val_acc: 0.3190\n",
      "Epoch 87/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.2992 - acc: 0.3110 - val_loss: 3.3162 - val_acc: 0.3001\n",
      "Epoch 88/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2964 - acc: 0.3131 - val_loss: 3.2354 - val_acc: 0.3141\n",
      "Epoch 89/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.3168 - acc: 0.3092 - val_loss: 3.3723 - val_acc: 0.2880\n",
      "Epoch 90/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.3372 - acc: 0.3078 - val_loss: 3.2484 - val_acc: 0.3097\n",
      "Epoch 91/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.3290 - acc: 0.3070 - val_loss: 3.1630 - val_acc: 0.3260\n",
      "Epoch 92/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.3422 - acc: 0.3031 - val_loss: 3.2593 - val_acc: 0.3136\n",
      "Epoch 93/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.3332 - acc: 0.3050 - val_loss: 3.2699 - val_acc: 0.3033\n",
      "Epoch 94/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.3587 - acc: 0.3036 - val_loss: 3.2130 - val_acc: 0.3203\n",
      "Epoch 95/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.3618 - acc: 0.3033 - val_loss: 3.2911 - val_acc: 0.2991\n",
      "Epoch 96/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.3883 - acc: 0.2991 - val_loss: 3.2519 - val_acc: 0.3081\n",
      "Epoch 97/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.4074 - acc: 0.2965 - val_loss: 3.2568 - val_acc: 0.3171\n",
      "Epoch 98/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.4432 - acc: 0.2927 - val_loss: 3.1682 - val_acc: 0.3221\n",
      "Epoch 99/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.4431 - acc: 0.2931 - val_loss: 3.4871 - val_acc: 0.2703\n",
      "Epoch 100/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.4654 - acc: 0.2885 - val_loss: 3.2561 - val_acc: 0.3169\n",
      "Epoch 101/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.4865 - acc: 0.2844 - val_loss: 3.2873 - val_acc: 0.3168\n",
      "Epoch 102/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.5280 - acc: 0.2784 - val_loss: 3.3587 - val_acc: 0.3069\n",
      "Epoch 103/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.5324 - acc: 0.2793 - val_loss: 3.2995 - val_acc: 0.3046\n",
      "Epoch 104/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.5373 - acc: 0.2794 - val_loss: 3.3116 - val_acc: 0.3043\n",
      "Epoch 105/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.5769 - acc: 0.2746 - val_loss: 3.2517 - val_acc: 0.3150\n",
      "Epoch 106/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.5874 - acc: 0.2702 - val_loss: 3.2958 - val_acc: 0.3083\n",
      "Epoch 107/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.6106 - acc: 0.2672 - val_loss: 3.4352 - val_acc: 0.2790\n",
      "Epoch 108/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.6261 - acc: 0.2634 - val_loss: 3.2668 - val_acc: 0.3092\n",
      "Epoch 109/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.6576 - acc: 0.2603 - val_loss: 3.3657 - val_acc: 0.2963\n",
      "Epoch 110/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.6477 - acc: 0.2636 - val_loss: 3.3810 - val_acc: 0.2946\n",
      "Epoch 111/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.6430 - acc: 0.2634 - val_loss: 3.2878 - val_acc: 0.3104\n",
      "Epoch 112/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.6653 - acc: 0.2616 - val_loss: 3.3460 - val_acc: 0.3039\n",
      "Epoch 113/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.6828 - acc: 0.2564 - val_loss: 3.6325 - val_acc: 0.2438\n",
      "Epoch 114/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.6889 - acc: 0.2578 - val_loss: 3.3321 - val_acc: 0.3018\n",
      "Epoch 115/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7082 - acc: 0.2567 - val_loss: 3.4115 - val_acc: 0.2946\n",
      "Epoch 116/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7409 - acc: 0.2470 - val_loss: 3.3786 - val_acc: 0.2958\n",
      "Epoch 117/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7532 - acc: 0.2468 - val_loss: 3.5013 - val_acc: 0.2818\n",
      "Epoch 118/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7402 - acc: 0.2497 - val_loss: 3.5057 - val_acc: 0.2660\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.7792 - acc: 0.2462 - val_loss: 3.4144 - val_acc: 0.2934\n",
      "Epoch 120/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7724 - acc: 0.2445 - val_loss: 3.5115 - val_acc: 0.2794\n",
      "Epoch 121/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.7684 - acc: 0.2453 - val_loss: 3.3796 - val_acc: 0.3017\n",
      "Epoch 122/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.7820 - acc: 0.2445 - val_loss: 3.3505 - val_acc: 0.2980\n",
      "Epoch 123/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7716 - acc: 0.2445 - val_loss: 3.4555 - val_acc: 0.2848\n",
      "Epoch 124/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7699 - acc: 0.2448 - val_loss: 3.4234 - val_acc: 0.2915\n",
      "Epoch 125/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7854 - acc: 0.2409 - val_loss: 3.5029 - val_acc: 0.2809\n",
      "Epoch 126/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7931 - acc: 0.2414 - val_loss: 3.6406 - val_acc: 0.2624\n",
      "Epoch 127/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8062 - acc: 0.2385 - val_loss: 3.3852 - val_acc: 0.2876\n",
      "Epoch 128/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8069 - acc: 0.2369 - val_loss: 3.8612 - val_acc: 0.2310\n",
      "Epoch 129/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8162 - acc: 0.2370 - val_loss: 3.4235 - val_acc: 0.2901\n",
      "Epoch 130/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8044 - acc: 0.2399 - val_loss: 3.4882 - val_acc: 0.2810\n",
      "Epoch 131/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8082 - acc: 0.2373 - val_loss: 3.5136 - val_acc: 0.2779\n",
      "Epoch 132/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8050 - acc: 0.2396 - val_loss: 3.4221 - val_acc: 0.2928\n",
      "Epoch 133/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.7902 - acc: 0.2427 - val_loss: 3.3640 - val_acc: 0.2990\n",
      "Epoch 134/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7883 - acc: 0.2410 - val_loss: 3.4694 - val_acc: 0.2835\n",
      "Epoch 135/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.8091 - acc: 0.2409 - val_loss: 3.4975 - val_acc: 0.2854\n",
      "Epoch 136/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7924 - acc: 0.2419 - val_loss: 3.4576 - val_acc: 0.2806\n",
      "Epoch 137/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7896 - acc: 0.2434 - val_loss: 3.4654 - val_acc: 0.2845\n",
      "Epoch 138/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7998 - acc: 0.2407 - val_loss: 3.4557 - val_acc: 0.2853\n",
      "Epoch 139/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8229 - acc: 0.2384 - val_loss: 3.4945 - val_acc: 0.2803\n",
      "Epoch 140/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8342 - acc: 0.2366 - val_loss: 3.6056 - val_acc: 0.2727\n",
      "Epoch 141/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8411 - acc: 0.2360 - val_loss: 3.4416 - val_acc: 0.2938\n",
      "Epoch 142/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8334 - acc: 0.2370 - val_loss: 3.4603 - val_acc: 0.2851\n",
      "Epoch 143/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8512 - acc: 0.2354 - val_loss: 3.5344 - val_acc: 0.2791\n",
      "Epoch 144/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8558 - acc: 0.2321 - val_loss: 3.5143 - val_acc: 0.2800\n",
      "Epoch 145/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8402 - acc: 0.2357 - val_loss: 3.4104 - val_acc: 0.2902\n",
      "Epoch 146/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8317 - acc: 0.2380 - val_loss: 3.3986 - val_acc: 0.3027\n",
      "Epoch 147/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8515 - acc: 0.2333 - val_loss: 3.5382 - val_acc: 0.2730\n",
      "Epoch 148/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8410 - acc: 0.2353 - val_loss: 3.3887 - val_acc: 0.3022\n",
      "Epoch 149/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8697 - acc: 0.2342 - val_loss: 3.4426 - val_acc: 0.2901\n",
      "Epoch 150/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8672 - acc: 0.2311 - val_loss: 3.4184 - val_acc: 0.2915\n",
      "Epoch 151/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8539 - acc: 0.2344 - val_loss: 3.4129 - val_acc: 0.2933\n",
      "Epoch 152/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8587 - acc: 0.2335 - val_loss: 3.5411 - val_acc: 0.2857\n",
      "Epoch 153/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8724 - acc: 0.2308 - val_loss: 3.6636 - val_acc: 0.2635\n",
      "Epoch 154/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8819 - acc: 0.2327 - val_loss: 3.4876 - val_acc: 0.2888\n",
      "Epoch 155/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8984 - acc: 0.2247 - val_loss: 3.4932 - val_acc: 0.2823\n",
      "Epoch 156/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8916 - acc: 0.2288 - val_loss: 3.4895 - val_acc: 0.2806\n",
      "Epoch 157/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9019 - acc: 0.2274 - val_loss: 3.4716 - val_acc: 0.2839\n",
      "Epoch 158/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8952 - acc: 0.2263 - val_loss: 3.6672 - val_acc: 0.2484\n",
      "Epoch 159/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9048 - acc: 0.2267 - val_loss: 3.4639 - val_acc: 0.2910\n",
      "Epoch 160/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9198 - acc: 0.2266 - val_loss: 3.5923 - val_acc: 0.2684\n",
      "Epoch 161/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9632 - acc: 0.2171 - val_loss: 3.5288 - val_acc: 0.2726\n",
      "Epoch 162/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9383 - acc: 0.2236 - val_loss: 3.4586 - val_acc: 0.2866\n",
      "Epoch 163/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9448 - acc: 0.2239 - val_loss: 3.5872 - val_acc: 0.2694\n",
      "Epoch 164/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9363 - acc: 0.2215 - val_loss: 3.5148 - val_acc: 0.2865\n",
      "Epoch 165/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9554 - acc: 0.2176 - val_loss: 3.5483 - val_acc: 0.2785\n",
      "Epoch 166/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9457 - acc: 0.2199 - val_loss: 3.5877 - val_acc: 0.2641\n",
      "Epoch 167/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9448 - acc: 0.2192 - val_loss: 3.5962 - val_acc: 0.2571\n",
      "Epoch 168/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.9637 - acc: 0.2207 - val_loss: 3.4965 - val_acc: 0.2970\n",
      "Epoch 169/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9567 - acc: 0.2179 - val_loss: 3.5305 - val_acc: 0.2808\n",
      "Epoch 170/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9781 - acc: 0.2162 - val_loss: 3.9689 - val_acc: 0.2165\n",
      "Epoch 171/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 3.9743 - acc: 0.2191 - val_loss: 3.7423 - val_acc: 0.2328\n",
      "Epoch 172/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9704 - acc: 0.2183 - val_loss: 3.4769 - val_acc: 0.2897\n",
      "Epoch 173/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9930 - acc: 0.2135 - val_loss: 3.5356 - val_acc: 0.2748\n",
      "Epoch 174/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9971 - acc: 0.2135 - val_loss: 3.5625 - val_acc: 0.2590\n",
      "Epoch 175/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 4.0094 - acc: 0.2144 - val_loss: 3.5388 - val_acc: 0.2826\n",
      "Epoch 176/200\n",
      "1563/1562 [==============================] - 34s 21ms/step - loss: 4.0256 - acc: 0.2109 - val_loss: 3.4881 - val_acc: 0.2823\n",
      "Epoch 177/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0389 - acc: 0.2103 - val_loss: 3.5353 - val_acc: 0.2631\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0219 - acc: 0.2105 - val_loss: 3.5772 - val_acc: 0.2700\n",
      "Epoch 179/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0367 - acc: 0.2084 - val_loss: 3.5627 - val_acc: 0.2705\n",
      "Epoch 180/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0551 - acc: 0.2096 - val_loss: 3.5709 - val_acc: 0.2755\n",
      "Epoch 181/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1054 - acc: 0.2045 - val_loss: 3.7939 - val_acc: 0.2733\n",
      "Epoch 182/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0745 - acc: 0.2052 - val_loss: 3.5694 - val_acc: 0.2693\n",
      "Epoch 183/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0852 - acc: 0.2032 - val_loss: 3.5907 - val_acc: 0.2688\n",
      "Epoch 184/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0839 - acc: 0.2036 - val_loss: 3.5897 - val_acc: 0.2704\n",
      "Epoch 185/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1041 - acc: 0.1988 - val_loss: 3.5900 - val_acc: 0.2640\n",
      "Epoch 186/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1112 - acc: 0.1993 - val_loss: 3.6537 - val_acc: 0.2624\n",
      "Epoch 187/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0601 - acc: 0.2061 - val_loss: 3.5798 - val_acc: 0.2720\n",
      "Epoch 188/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0687 - acc: 0.2075 - val_loss: 3.6519 - val_acc: 0.2645\n",
      "Epoch 189/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0577 - acc: 0.2081 - val_loss: 3.4755 - val_acc: 0.2780\n",
      "Epoch 190/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0833 - acc: 0.2018 - val_loss: 3.7194 - val_acc: 0.2501\n",
      "Epoch 191/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0771 - acc: 0.1990 - val_loss: 3.6485 - val_acc: 0.2514\n",
      "Epoch 192/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0796 - acc: 0.2005 - val_loss: 3.5808 - val_acc: 0.2653\n",
      "Epoch 193/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1068 - acc: 0.1979 - val_loss: 3.5339 - val_acc: 0.2769\n",
      "Epoch 194/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1094 - acc: 0.1977 - val_loss: 3.6463 - val_acc: 0.2591\n",
      "Epoch 195/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1094 - acc: 0.1961 - val_loss: 3.6353 - val_acc: 0.2639\n",
      "Epoch 196/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1223 - acc: 0.1964 - val_loss: 3.5916 - val_acc: 0.2688\n",
      "Epoch 197/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1359 - acc: 0.1980 - val_loss: 3.5715 - val_acc: 0.2739\n",
      "Epoch 198/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1252 - acc: 0.1943 - val_loss: 3.8185 - val_acc: 0.2494\n",
      "Epoch 199/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1386 - acc: 0.1921 - val_loss: 3.5886 - val_acc: 0.2608\n",
      "Epoch 200/200\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1565 - acc: 0.1924 - val_loss: 3.8348 - val_acc: 0.2278\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-535c2c4fd953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mmodel_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Score trained model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_base' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "num_classes = 200\n",
    "\n",
    "num_predictions = 20\n",
    "batch_size = 64\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "logs = \"logs/1\"\n",
    "tensorboard = TensorBoard(log_dir=logs)\n",
    "\n",
    "model_name = 'keras_imagenet200_100base.h5'\n",
    "\n",
    "#start = time()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', input_shape=train_images.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Conv2D(512, (3, 3)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(train_images)\n",
    "\n",
    "model.fit_generator(datagen.flow(train_images, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(train_images)/batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(val_images, y_test),\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 207us/step\n",
      "Test loss: 3.834818507385254\n",
      "Test accuracy: 0.2278\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(val_images, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "#print('Runtime:', str(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGDoptimizer\n",
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 5.1734 - acc: 0.0143 - val_loss: 4.9637 - val_acc: 0.0378\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.9299 - acc: 0.0366 - val_loss: 4.7335 - val_acc: 0.0667\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.7553 - acc: 0.0574 - val_loss: 4.5957 - val_acc: 0.0786\n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.6016 - acc: 0.0736 - val_loss: 4.3902 - val_acc: 0.1015\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.4454 - acc: 0.0919 - val_loss: 4.1839 - val_acc: 0.1312\n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.3163 - acc: 0.1087 - val_loss: 4.1917 - val_acc: 0.1282\n",
      "Epoch 7/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.2085 - acc: 0.1221 - val_loss: 4.0011 - val_acc: 0.1534\n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.1180 - acc: 0.1333 - val_loss: 3.9453 - val_acc: 0.1555\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 4.0396 - acc: 0.1442 - val_loss: 3.9148 - val_acc: 0.1693\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.9737 - acc: 0.1554 - val_loss: 3.7752 - val_acc: 0.1846\n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.9112 - acc: 0.1633 - val_loss: 3.7740 - val_acc: 0.1890\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.8509 - acc: 0.1714 - val_loss: 3.7557 - val_acc: 0.1879\n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.7995 - acc: 0.1814 - val_loss: 3.6331 - val_acc: 0.2041\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.7460 - acc: 0.1906 - val_loss: 3.5265 - val_acc: 0.2251\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.7018 - acc: 0.1954 - val_loss: 3.4928 - val_acc: 0.2308\n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.6573 - acc: 0.2031 - val_loss: 3.4223 - val_acc: 0.2421\n",
      "Epoch 17/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.6133 - acc: 0.2088 - val_loss: 3.4383 - val_acc: 0.2348\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.5787 - acc: 0.2165 - val_loss: 3.4249 - val_acc: 0.2379\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.5283 - acc: 0.2239 - val_loss: 3.3013 - val_acc: 0.2605\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.4983 - acc: 0.2295 - val_loss: 3.3942 - val_acc: 0.2517\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.4586 - acc: 0.2346 - val_loss: 3.2715 - val_acc: 0.2682\n",
      "Epoch 22/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.4283 - acc: 0.2398 - val_loss: 3.2714 - val_acc: 0.2609\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.3912 - acc: 0.2458 - val_loss: 3.2162 - val_acc: 0.2781\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.3605 - acc: 0.2508 - val_loss: 3.1493 - val_acc: 0.2966\n",
      "Epoch 25/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.3218 - acc: 0.2577 - val_loss: 3.1259 - val_acc: 0.2962\n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.2887 - acc: 0.2642 - val_loss: 3.1250 - val_acc: 0.2958\n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.2523 - acc: 0.2704 - val_loss: 3.0452 - val_acc: 0.3154\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.2235 - acc: 0.2765 - val_loss: 3.1124 - val_acc: 0.2984\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.1854 - acc: 0.2814 - val_loss: 3.0729 - val_acc: 0.3092\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.1554 - acc: 0.2876 - val_loss: 3.0176 - val_acc: 0.3165\n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.1289 - acc: 0.2911 - val_loss: 3.1348 - val_acc: 0.3041\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.0991 - acc: 0.2960 - val_loss: 2.9569 - val_acc: 0.3313\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.0660 - acc: 0.3015 - val_loss: 2.9634 - val_acc: 0.3287\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.0380 - acc: 0.3069 - val_loss: 2.9919 - val_acc: 0.3223\n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 3.0010 - acc: 0.3135 - val_loss: 2.9694 - val_acc: 0.3256\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.9690 - acc: 0.3202 - val_loss: 2.9704 - val_acc: 0.3306\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.9534 - acc: 0.3238 - val_loss: 2.8950 - val_acc: 0.3398\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.9160 - acc: 0.3289 - val_loss: 2.8321 - val_acc: 0.3485\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.8909 - acc: 0.3342 - val_loss: 2.8894 - val_acc: 0.3393\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.8578 - acc: 0.3390 - val_loss: 2.8742 - val_acc: 0.3469\n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.8322 - acc: 0.3435 - val_loss: 2.7998 - val_acc: 0.3594\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.8136 - acc: 0.3456 - val_loss: 2.7955 - val_acc: 0.3546\n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.7806 - acc: 0.3529 - val_loss: 2.7680 - val_acc: 0.3694\n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.7561 - acc: 0.3557 - val_loss: 2.7555 - val_acc: 0.3695\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.7260 - acc: 0.3619 - val_loss: 2.8035 - val_acc: 0.3590\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.7042 - acc: 0.3662 - val_loss: 2.7650 - val_acc: 0.3670\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.6824 - acc: 0.3720 - val_loss: 2.7556 - val_acc: 0.3687\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.6532 - acc: 0.3751 - val_loss: 2.7383 - val_acc: 0.3743\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.6289 - acc: 0.3813 - val_loss: 2.6910 - val_acc: 0.3827\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 32s 20ms/step - loss: 2.6056 - acc: 0.3839 - val_loss: 2.6995 - val_acc: 0.3789\n",
      "10000/10000 [==============================] - 2s 214us/step\n",
      "Test loss: 2.6994822227478026\n",
      "Test accuracy: 0.3789\n",
      "Runtime: 1591.073058128357\n",
      "\n",
      "\n",
      "\n",
      "Training RMSpropoptimizer\n",
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.8899 - acc: 0.0436 - val_loss: 4.4244 - val_acc: 0.1021\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.5408 - acc: 0.0853 - val_loss: 4.2048 - val_acc: 0.1371\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.4024 - acc: 0.1065 - val_loss: 4.0625 - val_acc: 0.1499\n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.2941 - acc: 0.1228 - val_loss: 3.9350 - val_acc: 0.1709\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.2141 - acc: 0.1349 - val_loss: 3.7940 - val_acc: 0.1952\n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1340 - acc: 0.1489 - val_loss: 3.6946 - val_acc: 0.2143\n",
      "Epoch 7/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0666 - acc: 0.1582 - val_loss: 3.6666 - val_acc: 0.2119\n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9878 - acc: 0.1694 - val_loss: 3.5781 - val_acc: 0.2272\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9297 - acc: 0.1793 - val_loss: 3.5860 - val_acc: 0.2258\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.8577 - acc: 0.1906 - val_loss: 3.4230 - val_acc: 0.2570\n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7909 - acc: 0.2025 - val_loss: 3.4205 - val_acc: 0.2567\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.7293 - acc: 0.2117 - val_loss: 3.3212 - val_acc: 0.2732\n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.6707 - acc: 0.2216 - val_loss: 3.3023 - val_acc: 0.2758\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.6090 - acc: 0.2320 - val_loss: 3.2302 - val_acc: 0.2916\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.5643 - acc: 0.2378 - val_loss: 3.2214 - val_acc: 0.2897\n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.5044 - acc: 0.2494 - val_loss: 3.1773 - val_acc: 0.3081\n",
      "Epoch 17/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.4522 - acc: 0.2579 - val_loss: 3.1673 - val_acc: 0.3083\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.4197 - acc: 0.2649 - val_loss: 3.1138 - val_acc: 0.3158\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.3721 - acc: 0.2718 - val_loss: 3.1578 - val_acc: 0.3073\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.3349 - acc: 0.2799 - val_loss: 3.0919 - val_acc: 0.3170\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2999 - acc: 0.2844 - val_loss: 3.0522 - val_acc: 0.3218\n",
      "Epoch 22/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2693 - acc: 0.2900 - val_loss: 3.0502 - val_acc: 0.3240\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2391 - acc: 0.2959 - val_loss: 3.0106 - val_acc: 0.3359\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.2042 - acc: 0.3037 - val_loss: 3.0231 - val_acc: 0.3315\n",
      "Epoch 25/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.1845 - acc: 0.3064 - val_loss: 3.0118 - val_acc: 0.3339\n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.1717 - acc: 0.3105 - val_loss: 2.9796 - val_acc: 0.3417\n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.1377 - acc: 0.3150 - val_loss: 3.0414 - val_acc: 0.3349\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.1272 - acc: 0.3177 - val_loss: 3.0179 - val_acc: 0.3337\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.1083 - acc: 0.3229 - val_loss: 2.9734 - val_acc: 0.3472\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0887 - acc: 0.3258 - val_loss: 3.0091 - val_acc: 0.3407\n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0721 - acc: 0.3298 - val_loss: 3.0034 - val_acc: 0.3422\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0532 - acc: 0.3337 - val_loss: 2.9543 - val_acc: 0.3432\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0363 - acc: 0.3363 - val_loss: 3.0137 - val_acc: 0.3425\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0268 - acc: 0.3392 - val_loss: 2.9577 - val_acc: 0.3476\n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.0056 - acc: 0.3444 - val_loss: 3.0286 - val_acc: 0.3442\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9930 - acc: 0.3475 - val_loss: 2.9212 - val_acc: 0.3601\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9823 - acc: 0.3485 - val_loss: 2.9810 - val_acc: 0.3538\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9652 - acc: 0.3521 - val_loss: 2.9655 - val_acc: 0.3505\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9499 - acc: 0.3559 - val_loss: 2.9258 - val_acc: 0.3608\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9373 - acc: 0.3585 - val_loss: 2.9554 - val_acc: 0.3553\n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.9143 - acc: 0.3620 - val_loss: 2.9501 - val_acc: 0.3505\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8983 - acc: 0.3654 - val_loss: 3.0194 - val_acc: 0.3487\n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8916 - acc: 0.3666 - val_loss: 2.9376 - val_acc: 0.3601\n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8808 - acc: 0.3696 - val_loss: 2.9701 - val_acc: 0.3591\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8686 - acc: 0.3738 - val_loss: 2.9754 - val_acc: 0.3543\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8517 - acc: 0.3741 - val_loss: 2.9691 - val_acc: 0.3540\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8370 - acc: 0.3792 - val_loss: 2.9909 - val_acc: 0.3529\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8215 - acc: 0.3805 - val_loss: 2.9727 - val_acc: 0.3591\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8148 - acc: 0.3829 - val_loss: 2.9138 - val_acc: 0.3708\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 2.8034 - acc: 0.3841 - val_loss: 2.9347 - val_acc: 0.3651\n",
      "10000/10000 [==============================] - 2s 214us/step\n",
      "Test loss: 2.934705940246582\n",
      "Test accuracy: 0.3651\n",
      "Runtime: 1694.8043329715729\n",
      "\n",
      "\n",
      "\n",
      "Training Adagradoptimizer\n",
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0311 - acc: 0.0049 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 7/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 22/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 25/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0372 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 33s 21ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "10000/10000 [==============================] - 2s 214us/step\n",
      "Test loss: 16.0375048828125\n",
      "Test accuracy: 0.005\n",
      "Runtime: 1670.5442538261414\n",
      "\n",
      "\n",
      "\n",
      "Training Adadeltaoptimizer\n",
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 15.9774 - acc: 0.0048 - val_loss: 16.0375 - val_acc: 0.0050loss: 15.7984 - acc: - ETA: 27s - l - ETA: 25s - loss: 15.8150 - acc: 0.004 - - ETA: 20s - loss: 15.8866 - acc: 0.005 - ETA: 20s - loss: 15.8873 - acc: 0.0 - ETA: 19s -  - ETA: 1s - loss: 15.9749 - acc:  - ETA: 1s - loss: 15.9754 - acc: 0. - ETA: 1s - loss: 15.9755 - acc:  - E\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 15.9975 - acc: 0.0048 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0139 - acc: 0.0049 - val_loss: 16.0375 - val_acc: 0.0050.0137 - acc: \n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050TA: 29s - loss: 16.0174 - acc:  - ETA: 25s  - ETA: 23s - loss:   - ETA: 19s - loss: 16.0326 - acc: 0.005 - ETA: 19s - loss: 16.0326 - acc: - ET - ETA: 17s - loss: 16.0340 - acc: 0.0 - ETA: 16s - loss: 16.0337 - acc: 0.00 - \n",
      "Epoch 7/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050s - loss: 16.0413  - ETA: 32s - ETA: 24s \n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00501 - acc:  - ETA: 22s - loss: 16 - ETA: 13s \n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050s: 16.0371 - a\n",
      "Epoch 17/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 acc: 0. - ETA: 1s - loss: 16.0362 - ETA: 1s - loss: 16.0362 - acc:  - E\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0373 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 22/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0376 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0392 - acc: 0.0048 - val_loss: 16.0375 - val_acc: 0.0050cc:  - ETA: 0s - loss: 16.03\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0340 - acc: 0.0051 - val_loss: 16.0375 - val_acc: 0.0050: 16.0315 - acc: 0.0 - ETA: 24s - loss: 16.0304 - a - ETA: 18s - loss: 16 - ETA: 17s -  - ETA: 15s - loss: 16.0366 - acc: 0.005 - ETA: 15s - loss: 16.0369 - acc: 0.0 - ETA: - ETA: 7s - loss:\n",
      "Epoch 25/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0377 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050- ETA: 3s - loss: 16.0375 - - ETA: 2s - loss: 16.0379 - ETA: \n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0376 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00500 - ETA: 20s  - ETA - ETA: 3s - loss: 16.0352 - acc: 0. - ETA: 3s\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.005074 - acc: \n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 ETA: 27s - loss: 16.0393 - ac - ETA: 26s - loss: 16.0403 - ETA: 0s - loss: 16.0376 - acc: \n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 - acc: 0. - ETA: 0s - l\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050- loss: 16.0370 - acc: 0.005 - ETA: 20s - loss: 16.0370 - acc: - ETA: 19s - loss: 16.0377 - acc: - ETA:\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "10000/10000 [==============================] - 2s 215us/step\n",
      "Test loss: 16.0375048828125\n",
      "Test accuracy: 0.005\n",
      "Runtime: 1842.5969245433807\n",
      "\n",
      "\n",
      "\n",
      "Training Adamoptimizer\n",
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 7.2864 - acc: 0.0235 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 7/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 17/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 25/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 35s 22ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 35s 23ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "10000/10000 [==============================] - 2s 219us/step\n",
      "Test loss: 16.0375048828125\n",
      "Test accuracy: 0.005\n",
      "Runtime: 1761.8552310466766\n",
      "\n",
      "\n",
      "\n",
      "Training Adamaxoptimizer\n",
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.7945 - acc: 0.0522 - val_loss: 4.3812 - val_acc: 0.0934\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.4819 - acc: 0.0876 - val_loss: 4.2742 - val_acc: 0.1182\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.3617 - acc: 0.1048 - val_loss: 4.1567 - val_acc: 0.1301\n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.2958 - acc: 0.1147 - val_loss: 4.0428 - val_acc: 0.1489\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.2493 - acc: 0.1210 - val_loss: 4.0846 - val_acc: 0.1399\n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1998 - acc: 0.1280 - val_loss: 3.9358 - val_acc: 0.1633\n",
      "Epoch 7/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1663 - acc: 0.1332 - val_loss: 4.0081 - val_acc: 0.1585\n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1418 - acc: 0.1373 - val_loss: 3.8319 - val_acc: 0.1724\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.1165 - acc: 0.1419 - val_loss: 3.8502 - val_acc: 0.1757\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0940 - acc: 0.1452 - val_loss: 3.8373 - val_acc: 0.1855\n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0757 - acc: 0.1471 - val_loss: 3.7702 - val_acc: 0.1873\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0663 - acc: 0.1503 - val_loss: 3.7788 - val_acc: 0.1859\n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0413 - acc: 0.1534 - val_loss: 3.6350 - val_acc: 0.2110\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0270 - acc: 0.1557 - val_loss: 3.6840 - val_acc: 0.1974\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0169 - acc: 0.1575 - val_loss: 3.6817 - val_acc: 0.2012\n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0148 - acc: 0.1560 - val_loss: 3.7429 - val_acc: 0.1852\n",
      "Epoch 17/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0073 - acc: 0.1592 - val_loss: 3.7910 - val_acc: 0.1815\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0084 - acc: 0.1583 - val_loss: 3.6584 - val_acc: 0.2049\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0036 - acc: 0.1594 - val_loss: 3.8040 - val_acc: 0.1863\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0035 - acc: 0.1584 - val_loss: 3.7515 - val_acc: 0.1929\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9998 - acc: 0.1585 - val_loss: 3.6361 - val_acc: 0.2100\n",
      "Epoch 22/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0001 - acc: 0.1591 - val_loss: 3.9080 - val_acc: 0.1638\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0042 - acc: 0.1609 - val_loss: 3.6977 - val_acc: 0.2003\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0010 - acc: 0.1595 - val_loss: 3.7697 - val_acc: 0.1908\n",
      "Epoch 25/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9902 - acc: 0.1611 - val_loss: 3.7066 - val_acc: 0.1967\n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9940 - acc: 0.1598 - val_loss: 3.7459 - val_acc: 0.1947\n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 3.9961 - acc: 0.1619 - val_loss: 3.7179 - val_acc: 0.1928\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0055 - acc: 0.1607 - val_loss: 3.6922 - val_acc: 0.1962\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0021 - acc: 0.1608 - val_loss: 3.5972 - val_acc: 0.2139\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0087 - acc: 0.1616 - val_loss: 3.6861 - val_acc: 0.2039\n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0134 - acc: 0.1594 - val_loss: 3.7423 - val_acc: 0.1932\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0126 - acc: 0.1604 - val_loss: 3.7663 - val_acc: 0.1974\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0127 - acc: 0.1601 - val_loss: 3.6866 - val_acc: 0.1979\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0140 - acc: 0.1596 - val_loss: 3.6990 - val_acc: 0.1995\n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0285 - acc: 0.1590 - val_loss: 3.6959 - val_acc: 0.2000\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0281 - acc: 0.1603 - val_loss: 3.6730 - val_acc: 0.2044\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0247 - acc: 0.1586 - val_loss: 3.6780 - val_acc: 0.2021\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0202 - acc: 0.1593 - val_loss: 3.8473 - val_acc: 0.1743\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0372 - acc: 0.1567 - val_loss: 3.7285 - val_acc: 0.1886\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0351 - acc: 0.1591 - val_loss: 3.8977 - val_acc: 0.1661\n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0510 - acc: 0.1567 - val_loss: 3.6892 - val_acc: 0.1999\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0440 - acc: 0.1557 - val_loss: 3.7553 - val_acc: 0.1905\n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0575 - acc: 0.1549 - val_loss: 3.8911 - val_acc: 0.1722\n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0549 - acc: 0.1556 - val_loss: 3.7131 - val_acc: 0.1956\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0615 - acc: 0.1540 - val_loss: 3.8442 - val_acc: 0.1728\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0592 - acc: 0.1567 - val_loss: 3.6931 - val_acc: 0.1913\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0623 - acc: 0.1572 - val_loss: 3.7434 - val_acc: 0.1937\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0577 - acc: 0.1544 - val_loss: 3.7356 - val_acc: 0.1918\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0633 - acc: 0.1554 - val_loss: 3.7570 - val_acc: 0.1856\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 34s 22ms/step - loss: 4.0675 - acc: 0.1554 - val_loss: 3.7469 - val_acc: 0.1958\n",
      "10000/10000 [==============================] - 2s 224us/step\n",
      "Test loss: 3.7469262130737304\n",
      "Test accuracy: 0.1958\n",
      "Runtime: 1715.1234188079834\n",
      "\n",
      "\n",
      "\n",
      "Training Nadamoptimizer\n",
      "Epoch 1/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0248 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050oss: 16.0141 - acc: 0.005 - ETA: 14s - loss: 16.0139 - acc: 0.0 - ETA: 14s - loss: 16.0141 - acc: 0. - ETA: 13s - loss: 16.0138 - acc: 0.005 - ETA: 13s - loss: 16.0141 - ETA: 12s - loss: 16.0155 - acc: 0.0 - ETA: 12s - loss: 16.0154 - ETA: 11s - loss: 16.0159 - acc: 0.005 - ETA: 11s - loss: 16.0159 - acc: - ETA: 11s - los - ETA: 9s - loss: 16.0159 - acc:  - ETA: 9s - loss: 16.0161 - acc - ETA: 9s - loss: 16.0165 - - ETA: 8s - loss: 16.0174 - ETA: 8s - - ETA - ETA: 2s - loss: 16. - ETA: 0s - loss: 16.0249 - acc:  - ETA: 0s - loss: 16.0248 - acc: 0.00\n",
      "Epoch 2/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050.0052 - acc:  - ETA: 32s - loss: 16.0078 - - ETA: 31s - loss: 16.0181 - acc: - ETA: 31s - loss: 16.023 - ETA: 27s - loss: 16.0288 - acc: 0.0 - ETA: 27s - loss: 16.0282 - acc: - ETA: 26s - loss: 16.0292 - acc: 0 - ETA: 26s - loss: 16.0301 -  - ETA: 25s - loss: 16.0304 - ac - ETA: 25s - loss: 16.0301 - acc - ETA: 24s - loss: 16.030 - ETA: 23s -  - ETA: 21s - loss: 16.0372 - ETA: 20s - loss: 16.0358 - acc: 0.00 - ETA: 20s - loss: 16.0357 - acc: 0.0 - E - ETA: 18s - loss: 16.0385 - a - ETA: 17s - loss:  - ETA: 16s - loss: 16.0392 - acc - ETA: 15s - loss: 16 - ETA: 14s - loss: 16.0374 - - ETA: 13s - loss: 1  - E - ETA: 4s - loss: 16.0378 -\n",
      "Epoch 3/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00506.0397 - a - ETA: 32s - loss: 16.0488 - acc: 0. - ETA: 32s - loss: 16.0513 - a - ETA: 31s - loss: 16.0512 -  - ETA: 27s - loss: 16.0381 -  - ETA: 27s - loss: 16.0399 -  - ETA: 26s - loss: 16.0400 - - ETA: 25s - loss: 16.0370 - a - ETA: 24s - loss: 16.0373 - acc: - ETA: 24s - - ETA: 22s - loss: - ETA: 21s - loss: 16.0381 -  - ETA: 20s - loss: 16.0402 - acc:  - ETA: 19s - loss: 16.0400 - acc: 0. - ETA: 19s - loss: 16.0392 -  - ETA: 18s - \n",
      "Epoch 4/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.005030s - loss: 16.0410 - a - ETA: 29s - loss: 16.0372 - acc - ETA:  - ETA: 26s - loss: 16.0409 - acc: 0.0 - ETA: 26s - loss: 16.0414 - acc:  - ETA: 25s - loss: 16.0398 - acc: 0.004 - ET - ETA: 23s - loss: 16.040 - ETA: 22s - loss: 16.0378 - acc: 0.0 - ETA: 22s - loss: 16.0377 - acc: 0 - ETA: 21s - loss: 16.0376 -  - ETA: 21s - loss: 16.0374 - acc: 0.0 - ETA: 21s - loss: 16.0382 - acc: - ETA: 20s - loss:  - ETA: 19s - loss: 16.0402 - a - ETA: 18s - loss: 16.0410 - acc: 0.00 - ETA: 18s - loss: 16.040 - ETA: 17s - loss: 16.0409 - acc: 0. - ETA: 17s - loss: 16.0405 - - ETA: 16s - loss: 16.0390 - ac - ETA: 15s - loss: 16.0391 - ETA: 14s - loss: 16.0400 -  - ETA: 2s - loss: 16.03 - ETA:  - ETA: 0s - l\n",
      "Epoch 5/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 ETA: 30s - loss: 16.0368 - acc: 0 - ETA: 30s - los - ETA: 28s - loss: 16.0414 - acc: 0.004 - ETA: 28s - loss: 16.0413 - acc: - ETA: 28s - loss: 16.0425 - acc: 0. - ETA: 27s - loss: 16.0421 - acc - ETA: 27s - loss: 16.0393  - ETA: 26s - loss: 16.0401 - acc:  - ETA: 26s - loss: 16.0399 - acc: 0. - ETA: 25s - loss: 16.0 - ETA: 24s - loss: 16.0380 - acc: - ETA: 24s - loss: 16.0 - ETA: 23s - loss - ETA: 21s - loss: - ETA: 20s - loss: 16.0387 - acc: 0.004 - ETA: 19s - loss: 16.0383 - acc: 0. - ETA: 19s - loss: 16.0386 - acc: 0.00 - ETA: 19s - lo - ETA: 17s - loss: 16.04 - ETA: 16s - loss: 16.0410 - ac - ETA: 16s - loss: 16.0426 - acc: 0.004 - ETA: 16s - loss: 16.0425 - acc: 0.004 - ETA: 16s - loss: 16.0428 - acc: 0 - ETA: 15s - loss: 16. - ETA:  - ETA: 12s - loss: 16.0369 -  - ETA: 2s - loss: 16.0371 - acc - ETA: \n",
      "Epoch 6/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 - loss: 16.0254 - a - ETA: 29s - loss: 16.0254 - a - ETA: 29s - loss: 16.0274 - acc: - ETA: 28s - loss: 16.0270 - a - ETA: 27s - loss: 16.0309 - acc: 0.005 - ET - ETA: 22s - loss: 16.0351 - - ETA: 21s - loss: 16.0 - ETA: 20s - loss: 16.0345 - a - ETA: 20s - loss: 16.0 - ETA: 18s - loss: 16.0331 - acc: 0.005 - ETA: 18s - lo - ETA: 17s - loss: 16.0310  - ETA: 16s - loss: 16.0313 - acc - ETA: 1 - ETA: 13s - loss: 16.0327 - acc: 0.0 - ETA: 13s - loss: 16.0327 - acc: 0.005 - E - ETA: - E - ETA: 5s - loss: 16.0350 - ETA: 4s - loss: 16. - ETA: 4s - los - ETA: 3s - loss: 16.03 - ETA: 2s - loss: 16.0371 - a\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00506.0115 - - ETA: 26s - loss: 16.0303 - acc: 0. - ETA: 25s - loss: 16.0317 - acc: 0.00 - ETA: 25s - loss: 16.0317 - acc - ETA: 25s - loss: 16.0347 - acc: 0.0 - ETA: 25s - loss: 16.0353 - acc: 0.005 - ETA: 24s - loss: 16.0358 - acc: 0. - ETA: 24s - loss: 16.0352 -  - ETA: 23s - loss: 16.034 - ETA: 22s - loss: 16.0 - ETA: 21s - loss: 16.0354 - acc:  - ETA: 21s - loss: 16.0354  - ETA: 20s -  - ETA: 18s - loss: 16.0358 - acc: - ETA: 18s - loss: - ETA: 16s - loss: 16.0350 - acc: 0 - ETA: 16s - loss: 16.0356 - - ETA: 15s - loss: 16.0358 - acc:  - ETA: 15s - loss: 16.0360 - acc: 0.005 - ETA: 15s - loss: 16.0363 - acc: 0.005 - ETA: 15s - loss: 16 - ETA: 13s - loss: 16.0359 - acc: - ETA: 13s - loss: 16.0372  - ETA: 12s - loss: 16.0367 - acc: 0.0 - ETA: 12s - loss: 16.0370 - acc: 0 - ETA: 11s - loss: 16.0369 - acc: 0.005 - ETA: 11s - loss: 16.0372 - ac - ETA:  - ETA: 9s - loss: 16.0373 - ETA: 9s - loss: 16.0379 - - ETA: 8s - loss: 16.0376 - a - ETA: 8s - loss: 1 - ETA: 7s - l - ETA: 6s - loss: 16.0388 - acc: 0.00 - ETA: 6s - loss: - ETA: 5s - los - ETA\n",
      "Epoch 8/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 - loss: 16. - ETA: 32s - loss: 16.0321 - acc - ETA: 31s - l - ETA: 29s - loss: 16.03 - ETA: 28s - loss: - ETA: 27s - loss: 16.0327 - ac - ETA: 26s - loss: 16.0355 - - ETA: 17s - loss: - ETA: 16s - loss: 16.0337 - acc: 0.00 - ETA: 16s - loss: 1 - ETA: 14s - loss: 16.0360 - acc: - ETA: 14s - lo - ETA: 12s -  - ETA: 10s - loss: 16.0385 - acc: 0 - ETA: 10s - loss: 16.0396 - - ETA: 9s - loss: 16.0381 - acc:  - ETA: 0s - loss: 16.0377 - acc\n",
      "Epoch 9/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 - ETA: 30s - loss - ETA: 29s - loss: 16.0351  - ETA: 28s - loss: 16.0317 - acc: 0.005 - ETA: 28s - loss: 16.0309 - acc - ETA: 27s - loss: 16.0250 - a - ETA: 26s - loss: 16.0258 - ac - ETA: 26s - loss: 16 - ETA: 25s - loss: 16.0353 -  - ETA: 24s -  - ETA: 22s - loss: 16. - ETA: 21s - loss: 16.0416 - ETA: 17s - loss: 16.0406 - acc: 0.00 - ETA: 17s - los - ETA: 15s - loss: 16.0409 - acc: 0.  - ETA: 12s - loss: 16.0403 - acc - ETA: 12s - loss: 1 - ETA: 10s - loss: 16.0393 - acc: 0.004 - ETA: 10s - loss: 16.0393 - acc: 0.0 - ETA: 10s - loss: 16.0395 - acc: 0.0 - ETA: 6s - loss: 16.0375 - acc: 0. - ETA: 6s - loss: 16.0373 - acc:  - ETA: 5s - los - ETA - E - - ETA: 0s - loss: 16.0375 - acc: 0.00\n",
      "Epoch 10/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00503s - loss: 16.0274 - acc: 0. - ETA: 33s - loss: 16.0284 - acc: 0 - ETA: 32s - loss: 16.0317 - - ETA: 31s - loss: 16. - ETA: 30s - loss: 16.0368 - acc: 0. - ETA: 30s - loss: 16. - ETA: 29s - loss: 16.0 - ETA: 25s - loss: 16.0359 - ETA: 24s - loss: - ETA: 22s - loss: 16.0379 - acc: 0.00 - ETA: 22s - loss: 16.0378 - acc: 0.005 - ETA: 22s - loss: 16.0369 - acc: 0.0 - ETA: 22s - loss: 16.037 - ETA: 21s - loss: 16.0 - ETA: 20s - loss: 16.0380 - acc: 0.0 - ETA: 20s - loss: 16.0383 - acc: 0.0 - ETA: 19s - loss: 16 - ETA: 18s - loss: 16 - ETA: 17s - loss: 16.0412 - acc: 0.004 - ETA: 17s - loss: 16.0415 - acc: 0.00 - ETA: 17s - loss: 16.0414 - a - ETA: 16s - loss: 16.0409 - acc: - ETA: 16s - loss: 16.0413 - acc: 0. - ETA: 15s - l - ETA: 1 - ETA: 11s - los - ETA: 6s - loss: 16.0388 - ETA: 5s - loss: 1 - ETA: 4s - l\n",
      "Epoch 11/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 acc: 0. - ETA: 31s - loss: 16.0506 - acc - ETA: 31s - loss: 16.0 - ETA - ETA: 27s - loss: 16.0 - ETA: 23s - loss: 16.0434 - acc: 0.004 - ETA: 23s - loss: 16.0433 - acc: 0. - ETA: 23s - loss: 16.0451 - acc:  - ETA: 23s - loss - ETA: 21s - loss: 16.0456 - acc: 0.004 - ETA: 21s - loss: 16.0455 -  - ETA: 20s - loss: 16.046 - ETA: 19 - ETA: 17s - loss: 16.0445 - acc: 0.004 - ETA: 17s - loss - - ETA: 13s - loss: 16.0443 - ET - ETA: 10s - loss: 16.0393 - acc: 0.0 - ETA: 9s - loss: 16.0 - ETA: 9s - loss: 16. - ETA: 8s - ETA: 7s - loss: 16. - ETA: 7s - loss: 16. - ETA: 6s - loss: 16.0388 - acc: 0.00 - ETA:  - ETA: 0s - loss: 16.0375 - acc: 0.\n",
      "Epoch 12/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050: 34s - loss: 16.0341 - acc: 0.005 - ETA: 33s - loss: 16.0299 - ac - ETA: 33s - lo - ETA: 31s - loss: 16.032 - ETA: 30s - loss: 16.0350 - acc:  - ETA: 29s - l - ETA: 28s - loss: 16.0347 - ac - ETA: 27s - loss: 16.0347 - acc: 0.00 - ETA: 27s - loss:  - ETA: 26s - loss: 16.0358 - acc: 0. - ETA: 25s - loss: 16.0346 - acc: 0. - ETA: 25s - loss: 16.0357 - acc: 0.005 - ETA: 25s - lo  - ETA: 21s - loss:  - ETA: 19s - loss: 16.0 - ETA: 18s - ETA: 16s - loss: 16 - ETA: 15s - loss: 16.0394 - acc: - ETA: 14s - l - ETA: 13s - loss: 16.0381 - acc - ETA: 12s - los - ETA: 10s - loss: 16.0396 - acc: 0.0 - ETA: 10s - loss: 16.0393 - acc: 0. - ETA: 10s - loss: 16.0388 - acc: 0.004 - ETA: 10s - loss: 16.0386 - acc - ETA: 9s - loss: 16.0380 - acc: 0.00 - ETA: 9s - loss: 16.0383 - ETA: 6s - loss: 16.0364 - acc:  - ETA: 6s - loss: 1 - ETA: 4s - loss: 16.0370 - a - - ETA: 2s - loss: 16.0379 - acc: \n",
      "Epoch 13/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00500382 - acc: 0.00 - ETA: 25s - loss - ETA: 23s - loss: 16.0376 - acc:  - ETA: 23s - loss: 16.0384 - - ETA: 22s - loss: 16. - ETA: 21s - loss: 16.0382 - a - ETA: 20s - los - ETA: 18s - loss: 16.0429 - ETA: 17s - loss - ETA: 2s - loss: 16.0386 - acc: 0. - ETA: 2s - los\n",
      "Epoch 14/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050TA: 29s - loss: 16.0210 - a - ETA: 29s - loss: 16.0255 - acc: - ETA: 28s - loss: 16.0253 - acc: 0.005 - ETA: 28s -  - ETA: 26s - loss: 16.0327 - ac - ETA: 26s - loss: 16.0322 - a - ETA: 25 - ETA: 23s - loss: 16.0292 - ac - ETA: 23s - loss: - ETA: 21s - loss: 16.0302 - acc: - ETA: 21s - loss: 16.0299 - acc: 0 - ETA: 20s - loss: 16.0304 - acc: - ETA: 20s - loss: 16.0306 - acc: 0.005 - ETA: 20s - loss: 16.0302 - acc - ETA: 19s - lo - ETA: 1s - loss: - ETA: 1s - loss: 16.0364 - - ETA: 0s - loss: 16.0372 - acc: 0. - ETA: 0s - loss: 16.03\n",
      "Epoch 15/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 loss: 16.0341 - acc: 0. - ETA: 30s - loss: 16.0381 - acc: 0.00 - ETA: 30s - loss - ETA: - ETA: 26s - loss: 1 - ETA: 25s - loss: 16.0424 - acc:  - ETA: 25s - loss: 16.0421 - acc: 0.00 - ETA: 24s - loss: 16.0420 - ETA: 23s - loss: 16.0413 - acc:  - ETA: 23s - loss: 16. - ETA: 22s - ETA: 20s - loss: 16.0421 -  - ETA: 19s - loss - ETA: 18s - loss: 16.0360 - acc:  - ETA: 17s - loss: 16.0363 - acc: 0.00 - ETA: 17s - loss: 16.0366 - - ETA: 16s - loss: 16.0371 -  - ETA: 15s - loss: 16.0373 - a - ETA: 1 - ETA: 13s - loss: 16.0361 - acc: 0 - ETA: 12s - loss: 16.0364 - acc: 0. - ET - ETA: 10s - loss: 16.0386 - acc: 0.004 - ETA: 10s - loss: \n",
      "Epoch 16/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050A: 30s - loss: 16.0463 - acc: 0.00 - ETA: 30s - - ETA: 28s - loss: 16.0442 - acc: 0.004 - ETA: 28s - loss: 16.0 - ETA: 27s - loss: 16.0425 - acc - ETA: 16s - loss: 16.0365 - ETA: 15s - loss: 16.0361 - acc:  - ETA: 14s - lo - ETA: 12s - loss: 16.0352 - acc: 0. - ETA: 12s - loss: 16.0349 - acc: 0.0 - ETA: 12s - loss: 16.0349 - acc: 0.0 - ETA: 12s - loss: 16.0351 -  - ETA: 11s - loss: 16. - ETA: 10s - loss: 16.0353 - acc: 0.005 - ETA: 10s - loss: 16 - ETA: 8s - loss: 16.03 - - ETA: 6s - loss: 16. - ETA: 0s - loss: 16.0372 - acc:  - ETA: 0s - loss: 16.0373 - acc\n",
      "Epoch 17/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050A: 30s -  - ETA: 28s - loss: 16. - ETA: 27s - loss: - ETA: 23s - loss: 16.0402 - acc: - ETA: - ETA: 20s - lo - ETA: 18s - loss: 16.0384 - acc:  - ETA: 18s - loss: 16. - ETA: 17s - loss: 16.0387 - acc: 0. - ETA: 16s - los - ETA: 15s - loss: 16.04 -  - ETA: 11s - loss: 16.0370 - acc: 0.00 - ETA: 11s - loss: 16.0370 - acc:  - ETA: 11s - loss: 16.0367  - ETA: 10s - loss: 16.0355 - acc: 0.00 - ETA: 10s - loss: 16.0355  - ETA: 9s - los - ETA: 8s - loss: 16.03 - ETA: 8s - loss: 16.0377 - ETA: 7s - loss: 16.03 - ETA: 7s - loss: 16.0374 - acc:  - ETA: 7s - loss: 1 - ETA: 5s - ETA: 0s - loss: 16.0375 - acc: 0.00\n",
      "Epoch 18/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050- loss: 16.0461 - acc: 0 - ETA: 30s - loss: 16.0465 - a - ETA: 30s - loss: 16.0483 - acc: 0.0 - ETA: 30s - loss: 16.0432 - acc: - ETA: 29s - loss: 16.0 - ETA: 28s - loss: 16. - ETA: 21s - loss: 16.0414 - acc: 0. - ETA: 21s  - ETA: 16s - loss: 16.0400 - acc: 0 - ETA: 16s - loss: 16.0396 - acc: - ETA: 15s - loss: 16.0404 - acc - ETA: 15s - loss: 16.0399  - ETA: 14s - loss: 16.0377 - acc: 0.005 - ETA: 14s - loss: 16.0374 -  - ETA: 13s - loss: 16.0365 - acc: 0.005 - ETA: 13s - loss: 16.0360 - - ETA: 12s -  - ETA: 0s - loss: 16.03 - ETA: 0s - loss: 16.0378 - acc: 0.00 - ETA: 0s - loss: 16.0377 -\n",
      "Epoch 19/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 - loss: 16.0217 - acc - ETA: 29s - loss: 16.0397 - acc: 0 - ETA: 29s - loss: 16.0362 - acc:  - ETA: 29s - loss: 16.0370 - acc: 0.0 - ETA: 28s - loss: 16.0369 - acc: 0.00 - ETA: 28s - loss: 16.0332 - acc - ETA: 28s - loss: 16.0383 - acc: 0.00 - ETA: 28s - loss: 16.0366 - a - ETA: 27s - loss: 16.0 - ETA: 26s - loss: 16.0328 - a - ETA - ETA: 20s - loss: 16.036 - ETA: 16s - loss: 16.0410 - acc:  - ETA: 16s - loss: 16.0417 - acc: 0.00 - ETA: 16s - loss: 16.042 - ETA: 15s - loss: 16. - ETA: 13s - loss: 16.0419 - acc: 0.0 - ETA: 13s - loss: 1 - ETA: 12s - loss: 16.0422 - acc: 0. - ETA: 12s - loss: 16.0428 - acc:  - ETA: 11s - loss: 16.0432 - acc: 0 - ETA: 11s - loss: 16.0428 - acc - ETA: 10s - loss:  - ETA - ETA: 5s - loss: 16.0404 - ETA: 5s - loss: 16.0393 - acc - E - ETA: 1s - loss: 16.0368 - acc: 0. - ETA: 1s - loss: 16.0369 - a - ETA: 0s - l\n",
      "Epoch 20/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050oss: 16.0165 - acc: 0 - ETA: 33s - loss - ETA: 31s - - ETA: 29s - loss: 16.0353 - acc:  - ETA: 29s - loss: 16.0341 - acc: 0.0 - ETA: 29s - loss: 16.0341 - ETA: 28s - loss: 16.0350 - acc: - ETA: 27s - loss: 16.0373 - acc: - ETA: 27s - loss: 16.0386 - acc: 0.004 - ETA: 27s - loss: 16.0386 - acc: 0.004 - ETA: 27s - loss: 16.0393 - acc: 0.004 - ETA: 27s - loss: 16.0385 -  - ETA: 26s - loss: 16 - ETA: 19s - lo - ETA: 18s - loss: 16.0315 - a - ETA: 17s - loss: 16.0325 - ETA: 16s - loss: 16.0345 - acc:  - ETA: 16s - loss: 16.0347 - acc: 0 - ETA: 15s - loss: - ETA: 14s - loss: 16.0370 - - ETA: 13s - loss: 16. - ETA: 12s - loss: 16.03 - ETA: 11s - ETA: 9s - l - ETA: 8s - l - ETA: 6s - loss: 16.03 - ETA - ETA: 0s - los\n",
      "Epoch 21/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050- ETA: 31s - loss: 16.0347 - acc: 0.0 - ETA: 31s - loss: 16.0361 - acc: - ETA: - ETA: - ETA: 26s - loss: 16.0388 - acc: 0.00 - ETA: 26s - loss: 16.0388 - acc:  - ETA: 25s - loss: 16.0404 - acc: 0.004 - ETA: 25s - loss: 16.0397 - acc: - ETA: 25s - loss: 16.0389 - acc: 0.0 - ETA: 24s - loss: 16.0399 - acc: 0.00 - ETA: 24s - los - ETA: 20s - loss: 16.0393 - acc: - ETA: 19s - loss: 16.038 - ETA: 18s - loss: 16.0385 - acc: 0. - ETA: 18s - loss: 16.0377 - acc: - ETA: 18s - loss: 16.0386 - a - ETA: 17s - loss: 16.0404 - acc: 0. - ETA: 17s - loss: 16.0390 -  - ETA: 16s - loss: 16.03 - ETA: 12s - loss: 16.0349 - ETA: 11s - ETA: 8s - loss: 16.0350 - acc: 0.00 - ETA: 8s - loss: 16.0348 - acc:  - ETA: 8s - l - ETA: 7s - E - ETA: 3s - ETA: 1s - loss: 16.0375 - a - ETA: 1s - - ETA: 0s - loss: 16.0374 - acc: \n",
      "Epoch 22/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050s - lo - ETA: 29s - loss: 16.0432 - acc: 0.00 - ETA: 29s - loss: 16.0397 - acc: 0 - ETA: 29s - loss: 16.0383 - acc: 0 - ETA: 29s - loss: 16.0361 - acc: 0.0 - ETA: 28s - loss: 16.0370 - acc: - ETA: 28s - los - ETA: 26s - loss - ETA: 25s - loss: 16.0300 - acc: 0. - ETA: 25s - loss: 16.0301 - acc:  - ETA: 24s - loss: - ETA: 23s - loss: 16.02 - ETA: 22s - loss: 16.0293  - ETA: 21s - loss: 16.0309 - acc: 0.00 - - ETA: 18s - loss: 16.0351 - acc: 0.00 - ETA: 18s - loss: 16.0 - ETA: 17s - loss: 16 - ETA: 15s - loss: 16.0 - ETA: 14s - loss: 16.0327 - a - ETA: 14s - loss: 16.0332 - acc: 0 - ETA: 13s - loss: 16.0330 - acc:  - ETA: 13s - loss: 16.0341 - ETA: 12s - loss: 16.033 - ETA: 11s - loss: 16.0343 - acc: - ETA: 10s - loss: 16.0350 - acc - ETA: 10s  - ETA - ETA: 6s - - ETA: 1s - loss: 16. - ETA: 1s - l - ETA: 0s - loss: 16.0369 - acc - ETA: 0s - loss: 16.0373 - acc: 0.\n",
      "Epoch 23/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050loss: 16 - ETA: 29s - loss: 16.0526 - acc: 0 - ETA: 29s - loss: 16.0546 - ETA: 28s - loss: 16.050 - ETA: 27s - loss: 16.0507 - acc: 0.00 - ETA: 27s - loss: 16.0512 - acc: 0.00 - ETA: 27s - loss: 16.0501 - acc - ETA: 26s - loss: 16.0505 - acc: 0.0 - ETA: 26s - loss: 16.0501 - acc: 0.004 - ETA: 26s - loss: 16.0506 - a - ETA: 25s - loss: 16.0480 - acc: 0 - ETA: 25s - loss:  - ETA: 24 - ETA: 22s - loss: 16.0426  - ETA: 21s - loss: 16.0429 - acc: 0.004 - ETA: 21s - loss: 16.04 - ETA: 20s - loss: 16.0377 - acc - ETA: 19s - loss: 16.0375 - acc: 0.005 - ETA: 19s - loss: 16.0372 - ac - ETA: 18s - loss: 16.0378 - ac - ETA: 18s - loss: 16.0373 - a - ETA: 17s - loss:  - ETA: 16s - loss: 16. - ETA: 12s - loss: 16.0379 - acc: 0.005 - ETA: 12s - loss: 16.0376 - - ETA: 11s - loss: 16.0 - ETA: 10s - loss: 16.038 - ETA: 9s - loss: 16.0386 - acc:  - ETA: 5s - - ETA: 4s - ETA: 3s - ETA: 1s - loss: 16.0388 - acc:  - ETA: 0s - l\n",
      "Epoch 24/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050: 26s - loss: 16 - ETA: 25s - loss: 16.0324 - acc:  - ETA: 24s - loss: 16.0341 - acc: 0. - ETA: 24s - loss: 16.0347 - acc: 0. - ETA: 24s - loss: 16.0341 - acc - ET - ETA: 21s - loss: 16.0329 - acc: 0.00 - ETA: 21s - loss: 16.0337 - a - ETA: 20s - loss: 16.0349 - acc: - ETA: 20s - loss: 16.0349 - acc: 0.0 - ETA: 19s - loss: 16.0345 - a - ETA: 19s - loss: 16.0334 - ac - ETA: 18s - loss: 16.0335 - acc: 0.005 - ETA: 18s - loss: 16.0338 - acc: 0. - ETA: 18s - loss: 16.0341 - acc: 0.005 - ETA: 18s - - - ETA: 13s - loss: 16.0347 - acc: 0. - ETA: 13s - loss: 16.0355 - acc: - ETA - ETA: 8s - l\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050ETA: 29s - loss:  - ETA: 28s - - ETA: 26s - loss: 16.0335 - acc - ETA: 26s - loss: 16.0354 - acc:  - ETA: 25s - loss: 16.0354 - acc - ETA: 25s - loss: 16.0364 - acc: - ETA: 24s - loss: 16.0369 - acc: 0.005 - ETA: 24s - loss: 16.0374 - ETA: 23s - loss: 16.0346 - acc: 0.0 - ETA: 23s - loss: 16.0346 - acc: 0.00 - ETA: 23s - loss: 16.0351 -  - ETA: 22s - loss: 16.0351 - acc: 0.00 - ETA: 22s - loss: 16.0350 - ac - ETA: 21s - loss: 16.0353 - acc - ETA: 21s - loss: 16.0361 - acc: 0. - ETA: 21s - loss: 16.0348 - acc - ETA: 20s - loss: 16.0340 - acc: -  - ETA: 17s - loss: 16.03 - ETA: 16s - loss: 16.0 - ETA: 15s - loss:  - ETA: 11s - loss:  - ETA: 9s - - ETA: 8s - loss: 1 - ETA: 8s - loss: 16.0381 - acc: 0.00 - ETA: 8s - loss: 16.0383 - acc - ETA: 7s - loss: 16.0382 - a - ETA: 7s - loss: 16.0390 - a - ETA: 7s - loss: 1 - ETA: 6s - loss: 16. - ETA: 5s - loss: 16.0382 - acc: 0.00 - ETA: 5s - ETA: 4s - loss: 16.0378 - a - ETA: 4s - loss: 1 - ETA: 3s - loss: 16.0384 - acc - ETA: 3s - loss: 16.0384 - acc - ETA: 3s - loss: - ETA: 2s - loss: 16.0377 - acc:  - ETA: 2s - loss: 16.0375 - acc:  - ETA: 2s - loss: 16.0374 - acc: 0. - ETA: 2s - loss: 16.0373 - ETA: 1s - ETA: 0s - loss: 16.\n",
      "Epoch 26/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050s - ETA: 31s - loss: 16.0270 - acc: 0. - ETA:  - ETA: 29s - loss: 16 - ETA: 28s - loss: 16.0284 - ETA: 27s - loss: 16.0291 - acc: 0.0 - ETA: 26s - loss: 16.0286 - a - ETA: 26s - loss: 16.0270 - acc: 0 - ETA: 25s - loss: 16.02 - ETA: 24s - loss: 16.0 - ETA: 23s - loss: 16.0311 - acc: 0.0 - ETA: 23s - loss - ETA: 21s - loss: 16.0316 - acc:  - ETA: 21s - loss: 16.0304 -  - ETA: 20s - loss: 16.0345 - acc: 0.005 - ETA: 20s - loss: 16.0349 - acc: - ETA: 20s - loss: 16.0330  - ETA: 19s - loss: 16.0313 - acc: - ETA: 18s - loss: 16.0317 - acc: 0. - ETA: 18s - loss: 16.031 - ETA: 17s - loss: 16. - ETA: 16s - loss: - ETA: 14s - loss: 16.03 - ETA: 13s - loss: 16.0351 - - ETA: - ETA: 10s - loss: 16.0345 - acc: - ETA: 10s - loss: - ETA: 9s - loss: 16.0347 - a - ETA: \n",
      "Epoch 27/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0372 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 ETA: 30s - loss: 16.0307 - acc - ETA: 29s - loss: 16.0290 - ac - ETA: 28s - loss: 16.034 - ETA:  - ETA: 22s - loss: 16.0355 - acc: 0.00 - ETA: 22s - loss: 16.0355 - acc:  - ETA: 22s - l - ETA: 20s - loss: 16.039 - ETA: 19s - loss: 16.0389 - acc: 0 - ETA: 19s - loss: 16.0381  - ETA: 18s - loss: 16.0382 - acc:  - ETA: 17s - loss: 16.038 - ETA: 16s - loss: 16. - ETA: 15s - loss: 16.0388 - acc:  - ETA: 15s - loss: 16.0382 - acc: 0.00 - ETA: 15s - loss: 16.0379 - acc: 0.0 - ETA: 15s - loss: 16.0384 - acc: 0. - ETA: 14s - loss: 16.0386 - acc: 0.00 - ETA: 14s - loss: 16.0391 - acc: 0.0 - ETA: 14s - - ETA: 12s - loss: 16.0397 - acc: 0.00 - ETA: 12s - loss: 16.0396 - acc: 0.004 - ETA: 12s - loss: 16.0386 - - E - ETA: 8s - loss: 16.0389 - acc - ETA:  - ETA:  - ETA: 0s - loss: 16.0378 -\n",
      "Epoch 28/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00501 - acc: 0 - ETA: 32s - loss: 16.0541 - acc: - ETA: 31s - loss: 16.0530 - acc: 0. - ETA: 31 - ETA: 29s - loss: 16.0481 - acc - ETA: 28s - loss: 16.0439 - acc - ETA: 28s - l - ETA: 26s - loss: 16.0373 - acc:  - ETA: 26s - loss: 16.0378 - acc:  - ETA: 25s - loss: 16.0388 - acc: 0.004 - ETA: 25s - loss: 16.0388 - acc: - ETA: 25s - loss: 16.0397 - ac - E - ETA: 22s - loss: 16.0401 - acc - ETA: 21s  - ETA: 19s - loss: 16.0410 - acc: 0. - ETA: 19s - loss: 16.0412 - ETA: 18s - loss: 16.0405 - ac - ETA: 17s - loss: 16.0413 - acc: 0.0 - ETA: 17s - loss: 16.0409 - acc - ETA: 17s - loss: 16.0410 -  - ETA: 16s - loss: 16.0389 - acc: 0.0 - ETA: 16s - loss: 16.0388  - ETA: 15s - loss: 16.0386 - a - ETA: 14s - loss: 16. - ETA: 10s - loss: 16.0392 - E - ETA: 4s - loss: 16.0393 - ETA:  - ETA: 3s - loss: 16.0396 -\n",
      "Epoch 29/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 34s - loss: 16.0341 - acc: 0.005 - ETA: 34s - loss: 16.0237 - ac - ETA: - ETA: 31s - loss: 16.0384 - acc: 0.00 - ETA: 30s - loss: 16.0383 - ETA: 29s - loss: 16.0386 - - ETA: 29s - loss: 16 - ETA: 27s - loss: 16.0302 - acc: 0.0 - ETA: 27s - loss: 16 - ETA: 26s - loss: 16.0328  - ETA: 25s - los - ETA: 23s - loss - ETA: 22s - -  - ETA:  - ETA: 1 - ETA: 13s - loss: 16.0338 - acc: 0 - ETA: 13s - loss\n",
      "Epoch 30/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 16.0664 - acc: 0.003 - ETA: 32s - loss: 16.0635 - acc:  - ETA: 31s - loss: 16.0524  - ETA: 31s - loss: 16.0555 - acc: 0.0 - ETA: 30s - los - ETA: 29s - loss: 16.0400 - ac - ETA: 28s - loss: 16.04 - ETA: 27s  - ETA: 25s - loss: 16.0430 - acc: 0.0 - ETA: 25s - loss: 16.0434 - acc: 0.004 - ETA: 25s - loss: 16.0439 - acc: 0.0 - ETA: 25s - loss: 16.04 - ETA: 21s - loss:  - ETA: 17s - loss: 16.0416 - ETA: 16s - loss: 16.0416 - acc: 0. - ETA: 15s - loss: 16.0406 - acc: - ETA: 15s - loss: 16.0407 - acc: 0.00 - ETA: 15s - loss: 16.0404 - acc: 0.004 - ETA: 15s - loss: 16.0406 - acc: 0 - ETA: 12s - loss: 16 - ETA: 10s - loss: 16.0378 - acc: 0 - ETA: 10s - loss: 16.0 - ETA: 9s - - ETA: 8s - loss: 16.0384 - a - ETA: 8s - loss: - ETA: 4s - loss: 16.0388 - a - ETA: 4s - loss: 1 - ETA: 3s - loss: 16.03 - ETA: 3s - loss: 16. - ETA: 1s - loss: 16.0378 - acc: \n",
      "Epoch 31/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 16.0607 - ETA: 31s - loss: 16.0535 - a - ETA: 30s - loss: 16.0558 - acc:  - ETA: 30s - loss: 16.0477 - acc: 0 - ETA: 30s - loss: 16 - ETA: 28s - loss: 1 - ETA: 27s - los - ETA: 25s - loss: 16.0403 - acc: - ETA: 25s - loss: 16.0412 - ac - ETA: 24s - loss: 16.0402 - acc: 0.00 - ETA: 24s - loss: 16.0396 - acc: 0. - ETA: 24s - loss: 16.0400 - acc - ETA: 23s - loss: 16.0422 - acc: - ETA: 20s -  - ETA: 18s - loss: 16.043 - ETA: 1s - loss: 16.03 - ETA: 0s - - ETA: 0s - loss: 16.0374 - acc: 0.00\n",
      "Epoch 32/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.00507s - loss: 16.0305 - acc - ETA: 26s - loss: 16.03 - ETA: 2 - ETA: 0s - loss: 16.0380 - acc\n",
      "Epoch 33/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050: 30s -  - ETA: 28 - ETA: 26s - loss: 16.0420 -  - ETA: 26s - l - ETA: 24s - loss\n",
      "Epoch 34/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050l - ETA - ETA: 29s - loss: 16.0436 - acc - ETA: 28s - loss: 16 - ETA - ETA: 5s - loss: 16.0345 - acc: 0. - ETA: 4s - l - ETA: 4s - loss: 16.0350 - acc:  - ETA: 3s - loss: 16.0355 - acc - E - ETA: \n",
      "Epoch 35/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050loss: 16.0503 - acc: 0.00 - ETA - E - ETA: 23s - loss: 16.0434 - acc: 0.0 - ETA: 23s - - ETA: - ETA: 19s - loss: 16.04 - ETA: 17s - loss - ETA: 0s\n",
      "Epoch 36/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.005098 - acc:  - ETA: 30s - loss: 16.0302 - acc: 0.005 - ETA: 30s - loss: 16.0316 - a - ETA:  - ETA: 27s - loss: 16.0388 - - ETA: 26s - loss: 16.03 - ETA: 25s - loss: 16.0354 - acc: 0.005 - ETA: 25s - loss: 16.0360 - acc: 0 - - ETA: 22s - loss: 16.0332  - ETA: 2 - ETA: 19s - loss - ETA: 18s - loss: 16.0321 - acc - ETA: 17s - loss:  - ETA: 16s - loss: 16.0335 - acc: 0.00 - ETA: 16s - l - ETA: 14s - loss: 16.0 - ETA: 13s - loss: 16.0326 - ETA: 0s - loss: 16.0376 - a\n",
      "Epoch 37/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050- ETA: 29s - loss: 16 - ETA: 27s - loss: 16. - ETA: 26s - loss: 16.0362 - acc:  - ETA: 2 - ETA: 24s - loss: 16.0326  - ETA: 23s - loss: 16.0351 - acc: 0 - ETA: 22s - loss: - ETA: 18s - loss:  - ETA: 17s - loss: 16.0356 - acc: 0.005 - ET - ETA: 14s - loss: 16. - ETA: 13s - loss: 16.0367 - acc: 0.0 - ETA: 13s - loss: 16.0372 - ETA: 12s - loss: 16.0378 - acc: 0. - ETA: 12s - loss: 16.037 - ETA: 11s - loss: - ETA: 8s - loss: 16.0375 - acc: 0.00 - ETA: 8s - loss: - ETA: 2s - loss: 16.0363 - ETA: 1s - loss: 16.0366 - acc: 0.00 - ETA: 1s - loss: 16.0366 -\n",
      "Epoch 38/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050.0 - ETA: 29s - loss - ETA: 27s - loss: 1 - ETA: 26s - loss: 16.0374 - ETA: 25s - loss: 16.0365 - acc: 0.00 - ETA: 25s - loss: 16.0365 - acc: 0 - ETA: 2 - ETA: 22s - loss: 16.0341 - a - ETA: 22s - loss: 16.03 - ETA: 20s - loss: 16.0350 - acc: 0.0 - ETA: 20s - loss: 16.0357 - acc:  - ETA - ETA: 18s - loss: 16.0388 - acc: 0.004 - ETA: 17s - loss: 16.0385  - ETA: 17s - loss: 16.03 - ETA: 16s - loss: 16 - ETA: 14s - lo - ETA: 13s - loss\n",
      "Epoch 39/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050oss: 16.0366 - acc: 0.0 - ETA: 30s  - ETA: 28s - loss: 1 - ET - ETA: 24s - - ETA: 22s -  - ETA: 20s - loss: 16.0376 - acc: 0.00 - ETA: 20s - loss: 16. - ETA: 19s - loss: 16.0387 - acc: 0.004 - ETA: 19s - loss: 16. - ETA: 3s - loss: 16. - ETA: 2s - loss: 16.0367 - acc: 0. - ETA: 2s - loss: 16.0367 - a - ETA: 2s - loss:\n",
      "Epoch 40/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050.034 - ETA: 32s - loss: 16.0505 - ac - ETA: 31s - ETA: 29s - loss: 16.0488 - acc: 0.0 - ETA: 29s - loss: 16.0513 - acc: - ETA: 29 - ETA: 26s - loss: 16.0569 - acc: 0. - ETA: 26s - l - ETA -\n",
      "Epoch 41/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 28s - loss: 16.0341 - acc: 0. - ETA: 28s - loss: 16.0358 - ETA: 27s - loss: 16.0371 - acc: 0.00 - ETA: 24s - loss: 16.0368 - acc: 0.005 - ETA: 24s - loss: 16.0363 - acc: - ETA: 23s - loss: 16.0362 -  - ETA: 23s - l - ETA: 21s - l - ETA: 19s - loss: 1 - ETA: 18s - loss: 16.0358 - acc - ETA: 17s - loss: 16. - ETA: 16s - loss: 16.036 - ETA: 15s - loss: 16.0 - ETA: 14s - loss: 16.0363 - acc: 0.00 - ETA: 14s - loss: 16. - ETA: 12s - lo - E - ETA: 6s - loss: 16.0381 - - ETA: 6s - l - ETA: 5s - loss: 16.0387 - a - ETA: 5s - loss: 16.0387 - - ETA: 4s - loss: 16.0388 - - ETA: 4s - loss: 16.0384 - acc: 0.00 - ETA: 4s - loss: 16.0385 - - ETA\n",
      "Epoch 42/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050A: 32s - loss: 16.0439 - acc: - ETA: 32s - loss: 16.0386 - acc:   - ETA: 17s - los - ETA: 13s - loss: 16.038 - ETA: 12s - loss: 16.0 - ETA: 11s - loss:  - ETA: 9s - loss: 16.0401 - acc: 0.0 - ETA: 1s - loss: 16.0393 - acc: 0.00 - ETA: 1s - loss: 16.0391 - acc: 0. - ETA: 1s - loss: 16.0393 - acc - ETA: 1s - loss: 16.0391 - acc - ETA: \n",
      "Epoch 43/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 loss: 16.0573 - ac - ETA: 30s - loss: 16.0530 - acc: 0.0 - ETA: 30s - loss:  - ETA: 28s - loss: 16.0417 - acc: 0. - ETA - ETA: 26s - loss: 16.0420 - acc: 0. - ETA: 25s - loss - ETA: 21s - loss: 16.0410 - ETA: 20s - loss: 16.0406 - acc:  -  - ETA: 17s - loss: 16.0398 - acc:\n",
      "Epoch 44/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 45/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050- - E - ETA: 23s - loss: 16.0380 - acc: - ETA: 22s - loss: 16.0369 -  - E - ETA: 19s - loss: 16.0382 - acc:  - ETA: 19s - loss: 16.0366 - acc:  - ETA: 18s - ETA: 16s - loss: 16.0388 - acc: - ETA: 16s - loss: 1 - ETA: 14s  - ETA: 13s - loss: 16.0359 - acc: - ETA: 12s - loss: 16.0358 - ac - ETA: 11s - loss:  - ETA: 2s - l - ETA: 1s - loss: 16.0384 - ETA: 0s - l\n",
      "Epoch 46/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050 - loss: 16.0283 - - ETA: 11s - loss: 16.0385 - acc: 0.0 - ETA: 11s - loss: 16.0383 - acc: 0. - ETA: 11s - loss: 16.0377 - a -  - ETA: 7s - loss: 1 - ETA: 7s - loss: 16.03 - E\n",
      "Epoch 47/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050: 16.0482 - - ETA: 26s - loss: 16.0461 - acc: 0.00 - ETA: 26s - loss:  - ETA: 22s - loss: 16.0416 - acc: 0.004 - ETA: 22s  - ETA: 20s - los - ETA: 13s - loss: 16.0368  - ETA: 8s - loss: 16.0366 -\n",
      "Epoch 48/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0374 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050\n",
      "Epoch 49/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050s: 16.054 - ETA: 29s  - ETA: 27s - loss: 16.0 - ETA: 26s - loss: 16.0579 - ETA: 25s  - ETA: 23s - loss: 16.0542 - acc: 0 - ETA: 23s - loss: 16.0551 - ac - ETA: 1s - loss: 16.03 - ETA: 0s - loss: 16.0372 - acc: 0.00 - ETA: 0s - l\n",
      "Epoch 50/50\n",
      "1563/1562 [==============================] - 37s 24ms/step - loss: 16.0375 - acc: 0.0050 - val_loss: 16.0375 - val_acc: 0.0050s: 16.0368 - acc: 0 - ETA: 26s - loss: 16 - ETA: 25s - loss: 16.03 - ETA: 24s - loss: 16.0329 - acc: - ETA: 24s - loss: 16.035 - ETA: 23s - loss: 16 - ETA: 16s - lo - ETA: 14s - loss: 16.0307 - acc: 0. - ETA: 14s - loss: 16.0310 - a - ETA: 13s - loss: 16.0319 - acc: 0.005 - ETA: 13s - los - ETA: 12s - loss: 16.03 - ETA: 11s - loss: 16.0340 - acc: 0.005 - ETA: 10s -  - ETA: 9s - loss: 16.0347 - ETA: 9s - loss: 16.03 - ETA: 8s - loss: - - ETA: 1s - loss: 16.0375 - acc:  - ETA: 0s - l - ETA: 0s - loss: 16.0374 - acc: 0.\n",
      "10000/10000 [==============================] - 2s 223us/step\n",
      "Test loss: 16.0375048828125\n",
      "Test accuracy: 0.005\n",
      "Runtime: 1858.8441054821014\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opts = [('SGD', SGD()), ('RMSprop', RMSprop(lr=0.0001, decay=1e-6)), ('Adagrad', Adagrad()), ('Adadelta', Adadelta()), \n",
    "        ('Adam', Adam()), ('Adamax', Adamax()), ('Nadam', Nadam())]\n",
    "\n",
    "for name, opt in opts:\n",
    "    \n",
    "    print('Training ' + name + 'optimizer')\n",
    "    epochs = 50\n",
    "    num_classes = 200\n",
    "\n",
    "    num_predictions = 20\n",
    "    batch_size = 64\n",
    "\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    logs = \"logs/opt_100/\" + name\n",
    "    tensorboard = TensorBoard(log_dir=logs)\n",
    "\n",
    "    model_name = name+'keras_imagenet200_100base.h5'\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', input_shape=train_images.shape[1:]))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Conv2D(512, (3, 3)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_images)\n",
    "\n",
    "    model.fit_generator(datagen.flow(train_images, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(train_images)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(val_images, y_test),\n",
    "                        callbacks=[tensorboard])\n",
    "    \n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(val_images, y_test, verbose=1)\n",
    "    end = time()\n",
    "    \n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    print('Runtime:', str(end-start))\n",
    "    \n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['elu', 'relu', 'softplus', 'softsign', 'softmax']\n",
    "\n",
    "for name, act in activations:\n",
    "    \n",
    "    print('Training ' + name + ' activation')\n",
    "    epochs = 50\n",
    "    num_classes = 200\n",
    "\n",
    "    num_predictions = 20\n",
    "    batch_size = 64\n",
    "\n",
    "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "    logs = \"logs/opt_100/\" + name\n",
    "    tensorboard = TensorBoard(log_dir=logs)\n",
    "    #earlystopping = EarlyStopping(monitor='val_loss')\n",
    "\n",
    "    model_name = name+'keras_imagenet200_100base.h5'\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', input_shape=train_images.shape[1:]))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Conv2D(512, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=SGD(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_images)\n",
    "\n",
    "    model.fit_generator(datagen.flow(train_images, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(train_images)/batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(val_images, y_test),\n",
    "                        callbacks=[tensorboard])\n",
    "    \n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(val_images, y_test, verbose=1)\n",
    "    end = time()\n",
    "    \n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    print('Runtime:', str(end-start))\n",
    "    \n",
    "    print('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
